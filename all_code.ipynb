{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1834969569F45B385F2B0BFDAD121E5",
    "mdEditEnable": false
   },
   "source": [
    "# 1 特征提取\n",
    "* 训练数据只用了前4亿（尽管有的特征全部提取了，但实际只用了4亿）\n",
    "* 一共10种特征（有的特征分成了多个部分）\n",
    "* 除了特征10，其余的训练集特征都是4亿存在一起的，特征10由于一下提取4亿内存会炸，所以分成了前2亿和后两亿\n",
    "\n",
    "## 1.1 基础特征\n",
    "### 1.1.1 长度相关按行提取特征\n",
    "按行提取训练集长度相关特征（由于是按行提取的，所以测试集提取代码几乎完全一样所以不再给出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "59A113B1C5494C9C8CD770D8B4F35AF7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "TRAIN_ROWS, TEST_ROWS=1000000000, 20000000\n",
    "save_dir = \"/home/kesci/work/feature\"\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "test_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "\n",
    "assert os.path.exists(save_dir)\n",
    "\n",
    "train_columns = ['query_id', 'query', 'query_title_id', 'title', 'label']\n",
    "test_columns = ['query_id', 'query', 'query_title_id', 'title']\n",
    "\n",
    "\n",
    "debug = False\n",
    "nrows = None\n",
    "if debug:\n",
    "    nrows = 20000\n",
    "    \n",
    "    \n",
    "print('-------------- 开始提取特征 --------------')\n",
    "since = time.time()\n",
    "fea_names = ['query_length', 'title_length', 'WordMatchShare', 'WordMatchShare_query', \n",
    "             'WordMatchShare_title', 'LengthDiff', 'LengthDiffRate', 'LengthRatio_qt', 'LengthRatio_tq' # 这四个根据前面的计算得到\n",
    "            ]\n",
    "print(\"一共%d个特征:\" % len(fea_names))\n",
    "print(fea_names, flush=True)\n",
    "name2idx = {fea_name:fea_names.index(fea_name) for fea_name in fea_names}\n",
    "all_data = np.zeros((TRAIN_ROWS, len(fea_names)), dtype=np.float32)\n",
    "\n",
    "\n",
    "with open(train_file_path) as train_csvfile:\n",
    "    for idx, line in tqdm(enumerate(csv.reader(train_csvfile))):\n",
    "        if debug and idx == nrows:\n",
    "            print(\"debug mode. break.\")\n",
    "            break\n",
    "\n",
    "        query = line[1].strip().split()\n",
    "        title = line[3].strip().split()\n",
    "\n",
    "        query_len, title_len = len(query), len(title)\n",
    "        all_data[idx, name2idx['query_length']] = query_len\n",
    "        all_data[idx, name2idx['title_length']] = title_len\n",
    "\n",
    "        query_words = {}\n",
    "        title_words = {}\n",
    "        for word in query:  # query\n",
    "            query_words[word] = query_words.get(word, 0) + 1\n",
    "        for word in title:  # title\n",
    "            title_words[word] = title_words.get(word, 0) + 1\n",
    "        share_term = set(query_words.keys()) & set(title_words.keys())\n",
    "\n",
    "         # -------------------- WordMatchShare --------------\n",
    "        n_shared_word_in_query = sum([query_words[w] for w in share_term])\n",
    "        n_shared_word_in_title = sum([title_words[w] for w in share_term])\n",
    "\n",
    "        all_data[idx, name2idx['WordMatchShare']] = (n_shared_word_in_query + n_shared_word_in_title) / (query_len + title_len)\n",
    "        all_data[idx, name2idx['WordMatchShare_query']] = n_shared_word_in_query / query_len\n",
    "        all_data[idx, name2idx['WordMatchShare_title']] = n_shared_word_in_title / title_len\n",
    "\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete with idx %d in %d min %d s.' % (idx, time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "\n",
    "print(\"根据原始特征计算长度差等特征...\")\n",
    "all_data[:, name2idx['LengthDiff']] = all_data[:,name2idx['query_length']] - all_data[:, name2idx['title_length']]\n",
    "all_data[:, name2idx['LengthDiffRate']] = \\\n",
    "         np.amin(all_data[:,[name2idx['query_length'],name2idx['title_length']]], axis=1) / \\\n",
    "         np.amax(all_data[:,[name2idx['query_length'],name2idx['title_length']]], axis=1)\n",
    "         \n",
    "all_data[:, name2idx['LengthRatio_qt']] = \\\n",
    "         all_data[:, name2idx['query_length']] / all_data[:, name2idx['title_length']]\n",
    "all_data[:, name2idx['LengthRatio_tq']] = \\\n",
    "         all_data[:, name2idx['title_length']] / all_data[:, name2idx['query_length']]\n",
    "\n",
    "\n",
    "print(all_data[:5, :])\n",
    "print(all_data.dtype)\n",
    "\n",
    "if not debug:\n",
    "    print(\"saving...\")\n",
    "    save_path = os.path.join(save_dir, 'feature_1_0.csv.gz')\n",
    "    np.savetxt(save_path, all_data, fmt=\"%f\", delimiter=\",\", header=\",\".join(fea_names), comments=\"\")\n",
    "#     all_data.to_csv(save_path, compression='gzip', index=False)\n",
    "    print(save_path, \"save done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84D35F8EFD3445C88026FC977CF7615D"
   },
   "source": [
    "### 1.1.2 TFIDF相关特征\n",
    "只用了前四亿的训练集加测试集的数据计算TFIDF, 最后提取的特征训练集和测试集分开存放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "AAB738C33A6D4D8D8D6D02823AD9CC6A"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "E4 = 400000000\n",
    "# TRAIN_ROWS = 1000000000\n",
    "TEST_A_ROWS, TEST_B_ROWS = 20000000, 100000000\n",
    "save_dir = \"/home/kesci/work/feature_4e\"\n",
    "assert os.path.exists(save_dir)\n",
    "\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "test_A_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "test_B_file_path = '/home/kesci/input/bytedance/bytedance_contest.final_2.csv'\n",
    "\n",
    "train_columns = ['query_id','query','query_title_id','title','label']\n",
    "test_columns = ['query_id','query','query_title_id','title']\n",
    "print(\"当前进程PID:\", os.getpid(), \"开始时间:\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "\n",
    "debug = False\n",
    "nrows = None\n",
    "if debug:\n",
    "    nrows = 1000000\n",
    "\n",
    "\n",
    "doc_set = set()\n",
    "\n",
    "print(\"train 4e...\", flush=True)\n",
    "train_df = pd.read_csv(train_file_path, names=train_columns, \n",
    "                        nrows=(nrows if debug else E4))[['query','title']]\n",
    "print(train_df.shape)\n",
    "for title in tqdm(train_df['title']):\n",
    "    doc_set.add(title)\n",
    "for query in tqdm(train_df['query']):\n",
    "    doc_set.add(query)\n",
    "del train_df\n",
    "gc.collect()\n",
    "    \n",
    "    \n",
    "print(\"test A...\", flush=True)\n",
    "test_A_df = pd.read_csv(test_A_file_path, names=test_columns, nrows=nrows)[['query','title']]\n",
    "test_A_df[\"title\"] = test_A_df[\"title\"].apply(lambda x: x.strip()) # 去掉测试集title最后的tab\n",
    "print(test_A_df.shape)\n",
    "for title in tqdm(test_A_df['title']):\n",
    "    doc_set.add(title)\n",
    "for query in tqdm(test_A_df['query']):\n",
    "    doc_set.add(query)\n",
    "del test_A_df\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print(\"test B...\", flush=True)\n",
    "test_B_df = pd.read_csv(test_B_file_path, names=test_columns, nrows=nrows)[['query','title']]\n",
    "test_B_df[\"title\"] = test_B_df[\"title\"].apply(lambda x: x.strip()) # 去掉测试集title最后的tab\n",
    "print(test_B_df.shape)\n",
    "for title in tqdm(test_B_df['title']):\n",
    "    doc_set.add(title)\n",
    "for query in tqdm(test_B_df['query']):\n",
    "    doc_set.add(query)\n",
    "del test_B_df\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "idf = {}\n",
    "doc_len = len(doc_set)\n",
    "print(\"一共有%d个unique文档.\" % doc_len)\n",
    "for doc in tqdm(doc_set):\n",
    "    for word in set(doc.split()):\n",
    "        idf[word] =  idf.get(word, 0) + 1\n",
    "\n",
    "if not debug:\n",
    "    idf_wordcount_path = os.path.join(save_dir, \"train_4e_testAB_idf_wordcount.json\")\n",
    "    with open(idf_wordcount_path, 'w') as f:\n",
    "        json.dump(idf, f)\n",
    "        print(idf_wordcount_path, \"saving done!\")\n",
    "    \n",
    "    del doc_set\n",
    "    gc.collect()\n",
    "    \n",
    "# print(\"----------------------加载idf---------------------\")\n",
    "# term_count_path = os.path.join(save_dir, \"train_4e_testAB_idf_wordcount.json\")\n",
    "# with open(term_count_path, 'r') as f:\n",
    "#     idf = json.load(f)\n",
    "        \n",
    "print(\"整个语料库term数:\", len(idf))\n",
    "doc_len = 149247529 + 1 # 一共有149247529个unique文档\n",
    "for word in idf:\n",
    "    idf[word] = np.log(doc_len / (idf[word] + 1.)) + 1\n",
    "    \n",
    "print('-------------- 开始提取特征 --------------')\n",
    "since = time.time()\n",
    "fea_names = ['TFIDFWordMatchShare', 'TFIDFWordMatchShare_query', 'TFIDFWordMatchShare_title']\n",
    "print(\"一共%d个特征:\" % len(fea_names))\n",
    "print(fea_names, flush=True)\n",
    "name2idx = {fea_name:fea_names.index(fea_name) for fea_name in fea_names}\n",
    "\n",
    "all_data = np.zeros((E4 + TEST_A_ROWS + TEST_B_ROWS, len(fea_names)), dtype=np.float32)\n",
    "\n",
    "print(\"train:\", flush=True)\n",
    "since = time.time()\n",
    "with open(train_file_path) as train_csvfile:\n",
    "    csv_reader = csv.reader(train_csvfile)\n",
    "    for index, line in tqdm(enumerate(csv_reader)):\n",
    "        query = line[1].strip().split()\n",
    "        title = line[3].strip().split()\n",
    "        \n",
    "        query_words = {}\n",
    "        title_words = {}\n",
    "        for word in query:  # query\n",
    "            query_words[word] = query_words.get(word, 0) + 1\n",
    "        for word in title:  # title\n",
    "            title_words[word] = title_words.get(word, 0) + 1\n",
    "        share_term = set(query_words.keys()) & set(title_words.keys())\n",
    "\n",
    "        # --------------------- TFIDFWordMatchShare ---------------------\n",
    "        sum_shared_word_in_query = sum([query_words[w] * idf.get(w, 0) for w in share_term])\n",
    "        sum_shared_word_in_title = sum([title_words[w] * idf.get(w, 0) for w in share_term])\n",
    "        sum_query_tol = sum(query_words[w] * idf.get(w, 0) for w in query_words)\n",
    "        sum_title_tol = sum(title_words[w] * idf.get(w, 0) for w in title_words)\n",
    "        sum_tol = sum_query_tol + sum_title_tol\n",
    "\n",
    "        all_data[index, name2idx['TFIDFWordMatchShare']] = (sum_shared_word_in_query + sum_shared_word_in_title) / sum_tol\n",
    "        all_data[index, name2idx['TFIDFWordMatchShare_query']] = sum_shared_word_in_query / sum_query_tol\n",
    "        all_data[index, name2idx['TFIDFWordMatchShare_title']] = sum_shared_word_in_title / sum_title_tol\n",
    "            \n",
    "        if index == (E4 - 1):\n",
    "            print(\"训练集前4亿提取完成!\", flush=True)\n",
    "            break\n",
    "        if debug and index == 100000 - 1:\n",
    "            print(\"debug mode. break\")\n",
    "            break\n",
    "        \n",
    "time_elapsed = time.time() - since\n",
    "print('complete with idx %d in %d min %d s.' % (index, time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "\n",
    "offset = index + 1\n",
    "print(\"test A (offset=%d):\" % offset, flush=True)\n",
    "since = time.time()\n",
    "with open(test_A_file_path) as test_A_csvfile:\n",
    "    csv_reader = csv.reader(test_A_csvfile)\n",
    "    for index, line in tqdm(enumerate(csv_reader)):\n",
    "        query = line[1].strip().split()\n",
    "        title = line[3].strip().split()\n",
    "        \n",
    "        query_words = {}\n",
    "        title_words = {}\n",
    "        for word in query:  # query\n",
    "            query_words[word] = query_words.get(word, 0) + 1\n",
    "        for word in title:  # title\n",
    "            title_words[word] = title_words.get(word, 0) + 1\n",
    "        share_term = set(query_words.keys()) & set(title_words.keys())\n",
    "\n",
    "        # --------------------- TFIDFWordMatchShare ---------------------\n",
    "        sum_shared_word_in_query = sum([query_words[w] * idf.get(w, 0) for w in share_term])\n",
    "        sum_shared_word_in_title = sum([title_words[w] * idf.get(w, 0) for w in share_term])\n",
    "        sum_query_tol = sum(query_words[w] * idf.get(w, 0) for w in query_words)\n",
    "        sum_title_tol = sum(title_words[w] * idf.get(w, 0) for w in title_words)\n",
    "        sum_tol = sum_query_tol + sum_title_tol\n",
    "\n",
    "        all_data[index+offset, name2idx['TFIDFWordMatchShare']] = (sum_shared_word_in_query + sum_shared_word_in_title) / sum_tol\n",
    "        all_data[index+offset, name2idx['TFIDFWordMatchShare_query']] = sum_shared_word_in_query / sum_query_tol\n",
    "        all_data[index+offset, name2idx['TFIDFWordMatchShare_title']] = sum_shared_word_in_title / sum_title_tol\n",
    "        \n",
    "        if debug and index == 100000 - 1:\n",
    "            print(\"debug mode. break\")\n",
    "            break\n",
    "time_elapsed = time.time() - since\n",
    "print('complete with idx %d in %d min %d s.' % (index, time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "\n",
    "offset = offset + index + 1\n",
    "print(\"test B:(offset=%d):\" % offset, flush=True)\n",
    "since = time.time()\n",
    "with open(test_B_file_path) as test_B_csvfile:\n",
    "    csv_reader = csv.reader(test_B_csvfile)\n",
    "    for index, line in tqdm(enumerate(csv_reader)):\n",
    "        query = line[1].strip().split()\n",
    "        title = line[3].strip().split()\n",
    "        \n",
    "        query_words = {}\n",
    "        title_words = {}\n",
    "        for word in query:  # query\n",
    "            query_words[word] = query_words.get(word, 0) + 1\n",
    "        for word in title:  # title\n",
    "            title_words[word] = title_words.get(word, 0) + 1\n",
    "        share_term = set(query_words.keys()) & set(title_words.keys())\n",
    "\n",
    "        # --------------------- TFIDFWordMatchShare ---------------------\n",
    "        sum_shared_word_in_query = sum([query_words[w] * idf.get(w, 0) for w in share_term])\n",
    "        sum_shared_word_in_title = sum([title_words[w] * idf.get(w, 0) for w in share_term])\n",
    "        sum_query_tol = sum(query_words[w] * idf.get(w, 0) for w in query_words)\n",
    "        sum_title_tol = sum(title_words[w] * idf.get(w, 0) for w in title_words)\n",
    "        sum_tol = sum_query_tol + sum_title_tol\n",
    "\n",
    "        all_data[index+offset, name2idx['TFIDFWordMatchShare']] = (sum_shared_word_in_query + sum_shared_word_in_title) / sum_tol\n",
    "        all_data[index+offset, name2idx['TFIDFWordMatchShare_query']] = sum_shared_word_in_query / sum_query_tol\n",
    "        all_data[index+offset, name2idx['TFIDFWordMatchShare_title']] = sum_shared_word_in_title / sum_title_tol\n",
    "        \n",
    "        if debug and index == 100000 - 1:\n",
    "            print(\"debug mode. break\")\n",
    "            break\n",
    "time_elapsed = time.time() - since\n",
    "print('complete with idx %d in %d min %d s.' % (index, time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "print(all_data[:5, :])\n",
    "print(all_data[-5:, :])\n",
    "print(all_data.shape)\n",
    "\n",
    "if not debug:\n",
    "    train_save_path = os.path.join(save_dir, 'feature_1_1_4e.csv.gz')\n",
    "    print(\"保存4亿训练集到%s...\" % train_save_path)\n",
    "    np.savetxt(train_save_path, all_data[:E4, :], fmt=\"%f\", delimiter=\",\", \n",
    "                header=\",\".join(fea_names), comments=\"\")\n",
    "                \n",
    "    test_A_save_path = os.path.join(save_dir, 'feature_1_1_4e_testA.csv.gz')\n",
    "    print(\"保存测试集A到%s...\" % test_A_save_path)\n",
    "    np.savetxt(test_A_save_path, all_data[E4:E4+TEST_A_ROWS, :], fmt=\"%f\", delimiter=\",\", \n",
    "                header=\",\".join(fea_names), comments=\"\")\n",
    "    print(\"save done!\")\n",
    "    \n",
    "    test_B_save_path = os.path.join(save_dir, 'feature_1_1_4e_testB.csv.gz')\n",
    "    print(\"保存测试集B到%s...\" % test_B_save_path)\n",
    "    np.savetxt(test_B_save_path, all_data[E4+TEST_A_ROWS:, :], fmt=\"%f\", delimiter=\",\", \n",
    "                header=\",\".join(fea_names), comments=\"\")\n",
    "    print(\"save done!\")\n",
    "\n",
    "    del all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A175B579FE2A40838B28D76E5614082C"
   },
   "source": [
    "## 1.2 NgramJaccard特征\n",
    "按行提取NgramJaccard特征: ['NgramJaccardCoef_1' 'NgramJaccardCoef_2' 'NgramJaccardCoef_3' 'NgramJaccardCoef_4']\n",
    "\n",
    "（由于是按行提取的，所以测试集提取代码几乎完全一样所以不再给出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "A7F1D6A0B6BC467581234A56104C17FA"
   },
   "outputs": [],
   "source": [
    "# base feature\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import csv\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/kesci/work/code\") \n",
    "from utils import *  # NgramUtil\n",
    "\n",
    "\n",
    "TRAIN_ROWS, TEST_ROWS=1000000000, 20000000\n",
    "save_dir = \"/home/kesci/work/feature\"\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "test_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "\n",
    "assert os.path.exists(save_dir)\n",
    "\n",
    "train_columns = ['query_id', 'query', 'query_title_id', 'title', 'label']\n",
    "test_columns = ['query_id', 'query', 'query_title_id', 'title']\n",
    "\n",
    "\n",
    "debug = False\n",
    "nrows = None\n",
    "if debug:\n",
    "    nrows = 100000\n",
    "\n",
    "print('-------------- 开始提取NgramJaccard特征 --------------')\n",
    "since = time.time()\n",
    "fea_names = ['NgramJaccardCoef_1', 'NgramJaccardCoef_2', 'NgramJaccardCoef_3', 'NgramJaccardCoef_4']\n",
    "print(\"一共%d个特征:\" % len(fea_names))\n",
    "print(fea_names, flush=True)\n",
    "name2idx = {fea_name:fea_names.index(fea_name) for fea_name in fea_names}\n",
    "all_data = np.zeros((TRAIN_ROWS, len(fea_names)), dtype=np.float32)\n",
    "\n",
    "with open(train_file_path) as train_csvfile:\n",
    "    for idx, line in tqdm(enumerate(csv.reader(train_csvfile))):\n",
    "        if debug and idx == nrows:\n",
    "            print(\"debug mode, break.\")\n",
    "            break\n",
    "        query = line[1].strip().split()\n",
    "        title = line[3].strip().split()\n",
    "\n",
    "        for n in range(1, 5):\n",
    "            query_ngrams = NgramUtil.ngrams(query, n)\n",
    "            title_ngrams = NgramUtil.ngrams(title, n)\n",
    "            all_data[idx, name2idx['NgramJaccardCoef_%d'%n]] = DistanceUtil.jaccard_coef(query_ngrams, \n",
    "                                                                                         title_ngrams)\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('complete with idx %d in %d min %d s.' % (idx, time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "\n",
    "print(all_data.shape)\n",
    "print(all_data[:5, :])\n",
    "print(all_data.dtype)\n",
    "\n",
    "if not debug:\n",
    "    print(\"saving...\")\n",
    "    save_path = os.path.join(save_dir, 'feature_2.csv.gz')\n",
    "    np.savetxt(save_path, all_data, fmt=\"%f\", delimiter=\",\", header=\",\".join(fea_names), comments=\"\")\n",
    "#     all_data.to_csv(save_path, compression='gzip', index=False)\n",
    "    print(save_path, \"save done!\")\n",
    "\n",
    "del all_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6D1E75C8A4F4029887CCDF4C1F75359"
   },
   "source": [
    "## 1.3 Levenshtein相关\n",
    "特征: [\"Levenshtein_ratio\", \"Levenshtein_distance\", \"query_title_common_words\", \"common_word_ratio\"]\n",
    "    \n",
    "（由于是按行提取的，所以测试集提取代码几乎完全一样所以不再给出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "73FE03C285454A55B4DB7D346A6C541A"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import Levenshtein\n",
    "from fuzzywuzzy import fuzz\n",
    "from difflib import SequenceMatcher\n",
    "from itertools import chain\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"/home/kesci/work/code\") \n",
    "# from utils import *  # NgramUtil\n",
    "\n",
    "TRAIN_ROWS, TEST_ROWS=1000000000, 20000000\n",
    "save_dir = \"/home/kesci/work/feature\"\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "test_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "\n",
    "assert os.path.exists(save_dir)\n",
    "\n",
    "train_columns = ['query_id', 'query', 'query_title_id', 'title', 'label']\n",
    "test_columns = ['query_id', 'query', 'query_title_id', 'title']\n",
    "\n",
    "\n",
    "debug = False\n",
    "nrows = None\n",
    "if debug:\n",
    "    nrows = 100000\n",
    "\n",
    "# =============三个函数====================\n",
    "def edit_distance(word1, word2):  # 计算编辑距离\n",
    "    len1 = len(word1)\n",
    "    len2 = len(word2)\n",
    "    dp = np.zeros((len1 + 1, len2 + 1))\n",
    "    for i in range(len1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len2 + 1):\n",
    "        dp[0][j] = j\n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            delta = 0 if word1[i - 1] == word2[j - 1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j - 1] + delta,\n",
    "                           min(dp[i - 1][j] + 1, dp[i][j - 1] + 1))\n",
    "    return dp[len1][len2]\n",
    "\n",
    "\n",
    "# ----- 提取特征 -----\n",
    "since = time.time()\n",
    "\n",
    "fea_names = [\"Levenshtein_ratio\", \"Levenshtein_distance_char\",\n",
    "# \"Levenshtein_distance\", # 这个跑起来太慢, 先不要了\n",
    "\"query_title_common_words\", \"common_word_ratio\"]\n",
    "print(\"一共%d个特征:\" % len(fea_names))\n",
    "print(fea_names, flush=True)\n",
    "name2idx = {fea_name:fea_names.index(fea_name) for fea_name in fea_names}\n",
    "all_data = np.zeros((TRAIN_ROWS, len(fea_names)), dtype=np.float32)\n",
    "\n",
    "with open(train_file_path) as train_csvfile:\n",
    "    for idx, line in tqdm(enumerate(csv.reader(train_csvfile))):\n",
    "        if debug and idx == nrows:\n",
    "            print(\"debug mode, break.\")\n",
    "            break\n",
    "        query_str = line[1].strip()\n",
    "        query = query_str.split()\n",
    "        title_str = line[3].strip()\n",
    "        title = title_str.split()\n",
    "\n",
    "        # 9000 row/s\n",
    "        all_data[idx, name2idx[\"Levenshtein_ratio\"]] = Levenshtein.seqratio(query, title)\n",
    "        # all_data[idx, name2idx[\"Levenshtein_distance\"]] = edit_distance(query, title) # 很慢\n",
    "        all_data[idx, name2idx[\"Levenshtein_distance_char\"]] = Levenshtein.distance(query_str, title_str)\n",
    "        common_words_len = len(set(query) & set(title))\n",
    "        all_data[idx, name2idx[\"query_title_common_words\"]] = common_words_len\n",
    "        all_data[idx, name2idx[\"common_word_ratio\"]]= common_words_len / min(len(query), len(title))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete with idx %d in %d min %d s.' % (idx, time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "print(all_data.shape)\n",
    "print(all_data[:5, :])\n",
    "print(all_data.dtype)\n",
    "\n",
    "if not debug:\n",
    "    print(\"saving...\")\n",
    "    save_path = os.path.join(save_dir, 'feature_3.csv.gz')\n",
    "    np.savetxt(save_path, all_data, fmt=\"%f\", delimiter=\",\", header=\",\".join(fea_names), comments=\"\")\n",
    "    #     all_data.to_csv(save_path, compression='gzip', index=False)\n",
    "    print(save_path, \"save done!\")\n",
    "\n",
    "del (all_data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E47719DEB86F44CB8CE65779E8435862"
   },
   "source": [
    "## 1.4 sequencematch相关\n",
    "特征4: sequencematch [\"lcsubstr_len\", \"lcseque_len\", \"longest_match_size\", \"longest_match_ratio\"]\n",
    "\n",
    "（由于是按行提取的，所以测试集提取代码几乎完全一样所以不再给出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "46D61AE50A9F4FF790569805A59DD5D2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import Levenshtein\n",
    "from fuzzywuzzy import fuzz\n",
    "from difflib import SequenceMatcher\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "TRAIN_ROWS, TEST_ROWS=1000000000, 20000000\n",
    "save_dir = \"/home/kesci/work/feature\"\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "test_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "\n",
    "assert os.path.exists(save_dir)\n",
    "\n",
    "train_columns = ['query_id', 'query', 'query_title_id', 'title', 'label']\n",
    "test_columns = ['query_id', 'query', 'query_title_id', 'title']\n",
    "\n",
    "\n",
    "debug = False\n",
    "nrows = None\n",
    "if debug:\n",
    "    nrows = 100000\n",
    "\n",
    "\n",
    "def lcsubstr_lens(s1, s2):  # 计算最长子串长度\n",
    "    m = [[0 for i in range(len(s2) + 1)]\n",
    "         for j in range(len(s1) + 1)]  #生成0矩阵，为方便后续计算，比字符串长度多了一列\n",
    "    mmax = 0  #最长匹配的长度\n",
    "    p = 0  #最长匹配对应在s1中的最后一位\n",
    "    for i in range(len(s1)):\n",
    "        for j in range(len(s2)):\n",
    "            if s1[i] == s2[j]:\n",
    "                m[i + 1][j + 1] = m[i][j] + 1\n",
    "                if m[i + 1][j + 1] > mmax:\n",
    "                    mmax = m[i + 1][j + 1]\n",
    "                    p = i + 1\n",
    "    return mmax\n",
    "\n",
    "\n",
    "def lcseque_lens(s1, s2):  # 计算最长子序列长度\n",
    "    # 生成字符串长度加1的0矩阵，m用来保存对应位置匹配的结果\n",
    "    m = [[0 for x in range(len(s2) + 1)] for y in range(len(s1) + 1)]\n",
    "    # d用来记录转移方向\n",
    "    d = [[None for x in range(len(s2) + 1)] for y in range(len(s1) + 1)]\n",
    "    for p1 in range(len(s1)):\n",
    "        for p2 in range(len(s2)):\n",
    "            if s1[p1] == s2[p2]:  #字符匹配成功，则该位置的值为左上方的值加1\n",
    "                m[p1 + 1][p2 + 1] = m[p1][p2] + 1\n",
    "                d[p1 + 1][p2 + 1] = 'ok'\n",
    "            elif m[p1 + 1][p2] > m[p1][p2 + 1]:  #左值大于上值，则该位置的值为左值，并标记回溯时的方向\n",
    "                m[p1 + 1][p2 + 1] = m[p1 + 1][p2]\n",
    "                d[p1 + 1][p2 + 1] = 'left'\n",
    "            else:  #上值大于左值，则该位置的值为上值，并标记方向up\n",
    "                m[p1 + 1][p2 + 1] = m[p1][p2 + 1]\n",
    "                d[p1 + 1][p2 + 1] = 'up'\n",
    "    (p1, p2) = (len(s1), len(s2))\n",
    "    s = []\n",
    "    while m[p1][p2]:  #不为None时\n",
    "        c = d[p1][p2]\n",
    "        if c == 'ok':  #匹配成功，插入该字符，并向左上角找下一个\n",
    "            s.append(s1[p1 - 1])\n",
    "            p1 -= 1\n",
    "            p2 -= 1\n",
    "        if c == 'left':  #根据标记，向左找下一个\n",
    "            p2 -= 1\n",
    "        if c == 'up':  #根据标记，向上找下一个\n",
    "            p1 -= 1\n",
    "    return len(s)\n",
    "\n",
    "\n",
    "# ----- 提取特征 -----\n",
    "since = time.time()\n",
    "fea_names = [\"lcsubstr_len\", \"lcseque_len\", \"longest_match_size\", \"longest_match_ratio\"]\n",
    "name2idx = {fea_name:fea_names.index(fea_name) for fea_name in fea_names}\n",
    "print(\"一共%d个特征:\" % len(fea_names))\n",
    "print(fea_names, flush=True)\n",
    "all_data = np.zeros((TRAIN_ROWS, len(fea_names)), dtype=np.float32)\n",
    "\n",
    "with open(train_file_path) as train_csvfile:\n",
    "    for idx, line in tqdm(enumerate(csv.reader(train_csvfile))):\n",
    "        if debug and idx == nrows:\n",
    "            print(\"debug mode, break.\")\n",
    "            break\n",
    "        query_str = line[1].strip()\n",
    "        query = query_str.split()\n",
    "        title_str = line[3].strip()\n",
    "        title = title_str.split()\n",
    "\n",
    "        # part2: 22000 row/s\n",
    "        all_data[idx, name2idx[\"lcsubstr_len\"]] = lcsubstr_lens(query, title)\n",
    "        all_data[idx, name2idx[\"lcseque_len\"]] = lcseque_lens(query, title)\n",
    "        sq = SequenceMatcher(a=query, b=title)\n",
    "        match = sq.find_longest_match(0, len(query), 0, len(title))\n",
    "        all_data[idx, name2idx[\"longest_match_size\"]] = match.size\n",
    "        all_data[idx, name2idx[\"longest_match_ratio\"]] = match.size / min(len(query), len(title))\n",
    "time_elapsed = time.time() - since\n",
    "print('complete with idx %d in %d min %d s.' % (idx, time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "print(all_data[:5, :])\n",
    "print(all_data.dtype)\n",
    "\n",
    "if not debug:\n",
    "    print(\"saving...\")\n",
    "    save_path = os.path.join(save_dir, 'feature_4.csv.gz')\n",
    "    np.savetxt(save_path, all_data, fmt=\"%f\", delimiter=\",\", header=\",\".join(fea_names), comments=\"\")\n",
    "#     all_data.to_csv(save_path, compression='gzip', index=False)\n",
    "    print(save_path, \"save done!\")\n",
    "\n",
    "del (all_data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C704016F6F81449F9AA2E17D15490DDC"
   },
   "source": [
    "## 1.5 Fuzzy相关（第一部分）\n",
    "特征5: Fuzzy part1 Fuzzy部分1: [\"fuzz_qratio\", \"fuzz_partial_ratio\"]\n",
    "        \n",
    "（由于是按行提取的，所以测试集提取代码几乎完全一样所以不再给出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "EF7F36B953E640C287A1CDC37AE648CB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import Levenshtein\n",
    "from fuzzywuzzy import fuzz\n",
    "from difflib import SequenceMatcher\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "TRAIN_ROWS, TEST_ROWS=1000000000, 20000000\n",
    "save_dir = \"/home/kesci/work/feature\"\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "test_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "\n",
    "assert os.path.exists(save_dir)\n",
    "\n",
    "train_columns = ['query_id', 'query', 'query_title_id', 'title', 'label']\n",
    "test_columns = ['query_id', 'query', 'query_title_id', 'title']\n",
    "\n",
    "\n",
    "debug = False\n",
    "nrows = None\n",
    "if debug:\n",
    "    nrows = 100000\n",
    "\n",
    "\n",
    "# ----- 提取特征 -----\n",
    "since = time.time()\n",
    "\n",
    "\n",
    "fea_names = [\"fuzz_qratio\", \n",
    "# \"fuzz_WRatio\", # 太慢, 去掉 \n",
    "\"fuzz_partial_ratio\"]\n",
    "print(\"一共%d个特征:\" % len(fea_names))\n",
    "print(fea_names, flush=True)\n",
    "name2idx = {fea_name:fea_names.index(fea_name) for fea_name in fea_names}\n",
    "all_data = np.zeros((TRAIN_ROWS, len(fea_names)), dtype=np.float32)\n",
    "\n",
    "with open(train_file_path) as train_csvfile:\n",
    "    for idx, line in tqdm(enumerate(csv.reader(train_csvfile))):\n",
    "        if debug and idx == nrows:\n",
    "            print(\"debug mode, break.\")\n",
    "            break\n",
    "        query_str = line[1].strip()\n",
    "        query = query_str.split()\n",
    "        title_str = line[3].strip()\n",
    "        title = title_str.split()\n",
    "\n",
    "        # part3: 8000 row/s\n",
    "        all_data[idx, name2idx[\"fuzz_qratio\"]] = fuzz.QRatio(query_str, title_str)\n",
    "        # all_data[idx, name2idx[\"fuzz_WRatio\"]] = fuzz.WRatio(query_str, title_str) # 慢\n",
    "        all_data[idx, name2idx[\"fuzz_partial_ratio\"]] = fuzz.partial_ratio(query_str, title_str)\n",
    "            \n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete with idx %d in %d min %d s.' % (idx, time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "print(all_data[:5, :])\n",
    "print(all_data.dtype)\n",
    "\n",
    "if not debug:\n",
    "    print(\"saving...\")\n",
    "    save_path = os.path.join(save_dir, 'feature_5.csv.gz')\n",
    "    np.savetxt(save_path, all_data, fmt=\"%f\", delimiter=\",\", header=\",\".join(fea_names), comments=\"\")\n",
    "#     all_data.to_csv(save_path, compression='gzip', index=False)\n",
    "    print(save_path, \"save done!\")\n",
    "\n",
    "del (all_data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "631C812D0A7B4D5E8294D9727041E41E"
   },
   "source": [
    "## 1.6 Fuzzy相关（第二部分）\n",
    "特征6: Fuzzy 部分2: [\"fuzz_partial_token_sort_ratio\",\"fuzz_token_set_ratio\", \n",
    "                 \"fuzz_token_sort_ratio\"]\n",
    "        \n",
    "（由于是按行提取的，所以测试集提取代码几乎完全一样所以不再给出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "E7720E33F15243508F46A3919E33491C"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import Levenshtein\n",
    "from fuzzywuzzy import fuzz\n",
    "from difflib import SequenceMatcher\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "TRAIN_ROWS, TEST_ROWS=1000000000, 20000000\n",
    "save_dir = \"/home/kesci/work/feature\"\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "test_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "\n",
    "assert os.path.exists(save_dir)\n",
    "\n",
    "train_columns = ['query_id', 'query', 'query_title_id', 'title', 'label']\n",
    "test_columns = ['query_id', 'query', 'query_title_id', 'title']\n",
    "\n",
    "\n",
    "debug = False\n",
    "nrows = None\n",
    "if debug:\n",
    "    nrows = 100000\n",
    "\n",
    "\n",
    "# ----- 提取特征 -----\n",
    "since = time.time()\n",
    "\n",
    "\n",
    "fea_names = [\n",
    "    # \"fuzz_partial_token_set_ratio\", # 太慢, 去掉\n",
    "\"fuzz_partial_token_sort_ratio\", \n",
    "             \"fuzz_token_set_ratio\", \"fuzz_token_sort_ratio\"]\n",
    "print(\"一共%d个特征:\" % len(fea_names))\n",
    "print(fea_names, flush=True)\n",
    "name2idx = {fea_name:fea_names.index(fea_name) for fea_name in fea_names}\n",
    "all_data = np.zeros((TRAIN_ROWS, len(fea_names)), dtype=np.float32)\n",
    "\n",
    "with open(train_file_path) as train_csvfile:\n",
    "    for idx, line in tqdm(enumerate(csv.reader(train_csvfile))):\n",
    "        if debug and idx == nrows:\n",
    "            print(\"debug mode, break.\")\n",
    "            break\n",
    "        query_str = line[1].strip()\n",
    "        query = query_str.split()\n",
    "        title_str = line[3].strip()\n",
    "        title = title_str.split()\n",
    "\n",
    "        # part4: 8000 row/s\n",
    "        # all_data[idx, name2idx[\"fuzz_partial_token_set_ratio\"]]  = fuzz.partial_token_set_ratio(query_str, title_str)\n",
    "        all_data[idx, name2idx[\"fuzz_partial_token_sort_ratio\"]]  = fuzz.partial_token_sort_ratio(query_str, title_str)\n",
    "        all_data[idx, name2idx[\"fuzz_token_set_ratio\"]] = fuzz.token_set_ratio(query_str, title_str)\n",
    "        all_data[idx, name2idx[\"fuzz_token_sort_ratio\"]] = fuzz.token_sort_ratio(query_str, title_str)\n",
    "\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete with idx %d in %d min %d s.' % (idx, time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "print(all_data[:5, :])\n",
    "print(all_data.dtype)\n",
    "\n",
    "if not debug:\n",
    "    print(\"saving...\")\n",
    "    save_path = os.path.join(save_dir, 'feature_6.csv.gz')\n",
    "    np.savetxt(save_path, all_data, fmt=\"%f\", delimiter=\",\", header=\",\".join(fea_names), comments=\"\")\n",
    "#     all_data.to_csv(save_path, compression='gzip', index=False)\n",
    "    print(save_path, \"save done!\")\n",
    "\n",
    "del (all_data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1C43A9D7955346069EA7EBC9C1333B46"
   },
   "source": [
    "## 1.7 熵相关\n",
    "特征7: 信息熵 [\"query_Entropy\", \"title_Entropy\", \"query_title_Entropy\", \"WordMatchShare_Entropy\"]\n",
    "\n",
    "（由于是按行提取的，所以测试集提取代码几乎完全一样所以不再给出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "735F72473DAB4D7786B3A63ADABC25B9"
   },
   "outputs": [],
   "source": [
    "# informationEntropy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import Levenshtein\n",
    "from difflib import SequenceMatcher\n",
    "from itertools import chain\n",
    "\n",
    "# TRAIN_ROWS, TEST_ROWS=20000, 20000\n",
    "# save_dir = \"./feature\"\n",
    "# train_file_path = './sample_data/train_data.sample'\n",
    "# test_file_path = './sample_data/train_data_nolabel.csv'\n",
    "\n",
    "TRAIN_ROWS, TEST_ROWS=1000000000, 20000000\n",
    "save_dir = \"/home/kesci/work/feature\"\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "test_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "\n",
    "assert os.path.exists(save_dir)\n",
    "\n",
    "train_columns = ['query_id', 'query', 'query_title_id', 'title', 'label']\n",
    "test_columns = ['query_id', 'query', 'query_title_id', 'title']\n",
    "\n",
    "\n",
    "debug = False\n",
    "nrows = None\n",
    "if debug:\n",
    "    nrows = 100000\n",
    "\n",
    "\n",
    "print(\" ----- 提取informationEntropy特征 -----\")\n",
    "since = time.time()\n",
    "fea_names = [\"query_Entropy\", \"title_Entropy\", \"query_title_Entropy\", \"WordMatchShare_Entropy\"]\n",
    "print(\"一共%d个特征:\" % len(fea_names))\n",
    "print(fea_names, flush=True)\n",
    "name2idx = {fea_name:fea_names.index(fea_name) for fea_name in fea_names}\n",
    "all_data = np.zeros((TRAIN_ROWS, len(fea_names)), dtype=np.float32)\n",
    "\n",
    "with open(train_file_path) as train_csvfile:\n",
    "    for idx, line in tqdm(enumerate(csv.reader(train_csvfile))):\n",
    "        if debug and idx == nrows:\n",
    "            print(\"debug mode, break.\")\n",
    "            break\n",
    "\n",
    "        query_words = {}\n",
    "        title_words = {}\n",
    "        query_title_words = {}\n",
    "        for word in line[1].strip().split():  # query\n",
    "            query_words[word] = query_words.get(word, 0) + 1\n",
    "            query_title_words[word] = query_title_words.get(word, 0) + 1\n",
    "        for word in line[3].strip().split():  # title\n",
    "            title_words[word] = title_words.get(word, 0) + 1\n",
    "            query_title_words[word] = query_title_words.get(word, 0) + 1\n",
    "\n",
    "        n_query_tol = sum(query_words.values())\n",
    "        n_title_tol = sum(title_words.values())\n",
    "        n_query_title_tol = sum(query_title_words.values())\n",
    "        all_data[idx, name2idx[\"query_Entropy\"]] = abs(sum(map(lambda x: x/n_query_tol * \\\n",
    "          math.log(x/n_query_tol,2),query_words.values())))\n",
    "        all_data[idx, name2idx[\"title_Entropy\"]] = abs(sum(map(lambda x: x/n_title_tol * \\\n",
    "          math.log(x/n_title_tol,2),title_words.values())))\n",
    "        all_data[idx, name2idx[\"query_title_Entropy\"]] = abs(sum(map(lambda x: x/n_query_title_tol * \\\n",
    "          math.log(x/n_query_title_tol,2),query_title_words.values())))\n",
    "\n",
    "        query_title_words_share = {}\n",
    "        for word in query_words:\n",
    "            if word in title_words:\n",
    "                query_title_words_share[word] = query_title_words_share.get(\n",
    "                    word, 0) + query_words[word]\n",
    "        for word in title_words:\n",
    "            if word in query_words:\n",
    "                query_title_words_share[word] = query_title_words_share.get(\n",
    "                    word, 0) + title_words[word]\n",
    "\n",
    "        all_data[idx, name2idx[\"WordMatchShare_Entropy\"]] = abs(sum(map(lambda x: x/n_query_title_tol * \\\n",
    "          math.log(x/n_query_title_tol,2),query_title_words_share.values())))\n",
    "\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete with idx %d in %d min %d s.' % (idx, time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "print(all_data[:5, :])\n",
    "print(all_data.dtype)\n",
    "\n",
    "if not debug:\n",
    "    print(\"saving...\")\n",
    "    save_path = os.path.join(save_dir, 'feature_7.csv.gz')\n",
    "    np.savetxt(save_path, all_data, fmt=\"%f\", delimiter=\",\", header=\",\".join(fea_names), comments=\"\")\n",
    "#     all_data.to_csv(save_path, compression='gzip', index=False)\n",
    "    print(save_path, \"save done!\")\n",
    "\n",
    "del (all_data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3068CA5C835A4090A7210CBAC0810C8F"
   },
   "source": [
    "## 1.8 转换率特征\n",
    "提取query 和 title的转换率cvr, query选取1个关键词，title选取2个关键词. 关键词通过TFIDF确定."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "B7147D411D304426A287911F5AF84798"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import gc, json, time, csv, os \n",
    "import copy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm \n",
    "print(\"当前进程PID:\", os.getpid(), \"开始时间:\", \n",
    "      time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))) \n",
    "\n",
    "save_dir = \"/home/kesci/work/feature_4e\"\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "testA_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "testB_file_path = '/home/kesci/input/bytedance/bytedance_contest.final_2.csv'\n",
    "\n",
    "train_columns = ['query_id','query','query_title_id','title','label']\n",
    "test_columns = ['query_id','query','query_title_id','title']\n",
    "assert os.path.exists(save_dir)\n",
    "print(\"当前进程PID:\", os.getpid(), \"开始时间:\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "\n",
    "E4 = 400000000\n",
    "TESTA_ROWS = 20000000\n",
    "TESTB_ROWs = 100000000\n",
    "\n",
    "print(\"----------------------加载idf---------------------\")\n",
    "term_count_path = '/home/kesci/work/feature_4e/train_4e_testAB_idf_wordcount.json'\n",
    "with open(term_count_path, 'r') as f:\n",
    "    word2count = json.load(f)\n",
    "\n",
    "print(\"整个语料库term数:\", len(word2count))\n",
    "doc_len = 149247529 + 1 # 一共有149247530个unique文档\n",
    "idf = {}\n",
    "for word in word2count:\n",
    "    idf[word] = np.log(doc_len / (word2count[word] + 1.)) + 1\n",
    "\n",
    "query_term_top1_map = {}  # 映射关系（建立字典hash）\n",
    "title_term_top2_map = {}\n",
    "\n",
    "# 提取train_data 的query 和 title 关键词\n",
    "train_query_term_top1 = np.zeros((E4,), dtype=np.int32)\n",
    "train_title_term_top2 = np.zeros((E4,), dtype=np.int32)\n",
    "with open(train_file_path) as csvfile:\n",
    "    for index, line in tqdm(enumerate(csv.reader(csvfile))):\n",
    "        if index >= E4: break\n",
    "        query = line[1].strip().split()\n",
    "        title = line[3].strip().split()\n",
    "        # for query\n",
    "        query_count = Counter(query)\n",
    "        query_score = {word: query_count[word]/sum(query_count.values())*idf[word] for word in query_count}\n",
    "        word_top1 = sorted(query_score.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "        if query_term_top1_map.get(word_top1) == None:\n",
    "            query_term_top1_map[word_top1] = len(query_term_top1_map)\n",
    "        train_query_term_top1[index] = query_term_top1_map[word_top1]\n",
    "        \n",
    "        # for title\n",
    "        title_count = Counter(title)\n",
    "        title_score = {word: title_count[word]/sum(title_count.values())*idf[word] for word in title_count}\n",
    "        title_score = sorted(title_score.items(), key=lambda x: x[1], reverse=True)\n",
    "        word_top2 = ' '.join(list(set([x[0] for x in title_score[:2]])))\n",
    "        if title_term_top2_map.get(word_top2) == None:\n",
    "            title_term_top2_map[word_top2] = len(title_term_top2_map)\n",
    "        train_title_term_top2[index] = title_term_top2_map[word_top2]\n",
    "\n",
    "#  提取testA_data 的query 和 title 关键词\n",
    "testA_query_term_top1 = np.zeros((TESTA_ROWS,), dtype=np.int32)\n",
    "testA_title_term_top2 = np.zeros((TESTA_ROWS,), dtype=np.int32)\n",
    "since = time.time()\n",
    "with open(testA_file_path) as csvfile:\n",
    "    for index, line in enumerate(csv.reader(csvfile)):\n",
    "        query = line[1].strip().split()\n",
    "        title = line[3].strip().split()\n",
    "        # for query\n",
    "        query_count = Counter(query)\n",
    "        query_score = {word: query_count[word]/sum(query_count.values())*idf[word] for word in query_count}\n",
    "        word_top1 = sorted(query_score.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "        if query_term_top1_map.get(word_top1) == None:\n",
    "            query_term_top1_map[word_top1] = len(query_term_top1_map)\n",
    "        testA_query_term_top1[index] = query_term_top1_map[word_top1]\n",
    "        \n",
    "        # for title\n",
    "        title_count = Counter(title)\n",
    "        title_score = {word: title_count[word]/sum(title_count.values())*idf[word] for word in title_count}\n",
    "        title_score = sorted(title_score.items(), key=lambda x: x[1], reverse=True)\n",
    "        word_top2 = ' '.join(list(set([x[0] for x in title_score[:2]])))\n",
    "        if title_term_top2_map.get(word_top2) == None:\n",
    "            title_term_top2_map[word_top2] = len(title_term_top2_map)\n",
    "        testA_title_term_top2[index] = title_term_top2_map[word_top2]\n",
    "        if (index+1) % 4000000 == 0:\n",
    "            print(index+1, 'step end. Time consumed:', time.time() - since)\n",
    "\n",
    "# 提取testB_data 的query and title的关键词\n",
    "testB_query_term_top1 = np.zeros((TESTB_ROWs,), dtype=np.int32)\n",
    "testB_title_term_top2 = np.zeros((TESTB_ROWs,), dtype=np.int32)\n",
    "since = time.time()\n",
    "with open(testB_file_path) as csvfile:\n",
    "    for index, line in enumerate(csv.reader(csvfile)):\n",
    "        query = line[1].strip().split()\n",
    "        title = line[3].strip().split()\n",
    "        # for query\n",
    "        query_count = Counter(query)\n",
    "        query_score = {word: query_count[word]/sum(query_count.values())*idf[word] for word in query_count}\n",
    "        word_top1 = sorted(query_score.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "        if query_term_top1_map.get(word_top1) == None:\n",
    "            query_term_top1_map[word_top1] = len(query_term_top1_map)\n",
    "        testB_query_term_top1[index] = query_term_top1_map[word_top1]\n",
    "        \n",
    "        # for title\n",
    "        title_count = Counter(title)\n",
    "        title_score = {word: title_count[word]/sum(title_count.values())*idf[word] for word in title_count}\n",
    "        title_score = sorted(title_score.items(), key=lambda x: x[1], reverse=True)\n",
    "        word_top2 = ' '.join(list(set([x[0] for x in title_score[:2]])))\n",
    "        if title_term_top2_map.get(word_top2) == None:\n",
    "            title_term_top2_map[word_top2] = len(title_term_top2_map)\n",
    "        testB_title_term_top2[index] = title_term_top2_map[word_top2]\n",
    "        if (index+1) % 4000000 == 0:\n",
    "            print(index+1, 'step end. Time consumed:', time.time() - since)\n",
    "\n",
    "# 查看与分析提取的结果\n",
    "print(train_query_term_top1[:10], train_title_term_top2[:10])\n",
    "print(testA_query_term_top1[:10], testA_title_term_top2[:10])\n",
    "print(testB_query_term_top1[:10], testB_title_term_top2[:10])\n",
    "\n",
    "print('train的query和title总量: ', len(train_query_term_top1), len(train_title_term_top2))\n",
    "print('train的query和title不重复总量: ', len(set(train_query_term_top1)), len(set(train_title_term_top2)))   \n",
    "# 之前1e(636341, 40104211)  之前4e(756051, 53963588)\n",
    "\n",
    "print('映射最大值:', max(query_term_top1_map.values()), max(title_term_top2_map.values()))\n",
    "print('映射大小：', len(query_term_top1_map), len(title_term_top2_map))\n",
    "\n",
    "print(len(set(testA_query_term_top1)), len(testA_query_term_top1), len(set(train_query_term_top1)))\n",
    "print(len(set(testA_title_term_top2)), len(testA_title_term_top2), len(set(train_title_term_top2)))\n",
    "# 之前1e(285389, 20000000, 636341)  之前4e(285310, 20000000, 756051)\n",
    "print(len(set(testB_query_term_top1)), len(testB_query_term_top1))\n",
    "print(len(set(testB_title_term_top2)), len(testB_title_term_top2))\n",
    "\n",
    "print('train与testA相交(query)：', len(set(testA_query_term_top1) & set(train_query_term_top1)))\n",
    "print('train与testA相交(title)：', len(set(testA_title_term_top2) & set(train_title_term_top2)))\n",
    "print('train与testB相交(query)：', len(set(testB_query_term_top1) & set(train_query_term_top1)))\n",
    "print('train与testB相交(title)：', len(set(testB_title_term_top2) & set(train_title_term_top2)))\n",
    "del title_term_top2_map\n",
    "gc.collect()\n",
    "\n",
    "# 转化为pandas数据框\n",
    "all_data = pd.DataFrame()\n",
    "all_data['query'] = np.concatenate((train_query_term_top1,testA_query_term_top1,testB_query_term_top1))\n",
    "all_data['title'] = np.concatenate((train_title_term_top2,testA_title_term_top2,testB_title_term_top2))\n",
    "label_4e=pd.read_csv(\"/home/kesci/work/feature/train_label.csv.gz\", nrows=E4, dtype=np.int32)[\"label\"].values\n",
    "all_data['label'] = np.concatenate((label_4e, np.array([-1]*(120000000))))\n",
    "\n",
    "del train_query_term_top1,testA_query_term_top1,testB_query_term_top1\n",
    "del train_title_term_top2,testA_title_term_top2,testB_title_term_top2\n",
    "gc.collect()\n",
    "\n",
    "# 将int64 转化为 int32\n",
    "for col in all_data.columns:\n",
    "    if str(all_data[col].dtypes) == 'int64':\n",
    "        all_data[col] = all_data[col].astype(np.int32)\n",
    "all_data.to_csv('nn_related/all_data_cvr.csv.gz', compression='gzip', index=False) # 以便后续使用\n",
    "\n",
    "# 关闭kernel,从这再开始\n",
    "all_data = pd.read_csv('nn_related/all_data_cvr.csv.gz')\n",
    "random_sector = np.random.randint(1, 6, size=(all_data.shape[0])).astype(np.int32)  # 1,2,3,4,5\n",
    "all_data['random_sector'] = random_sector\n",
    "all_data['random_sector'][400000000:] = 0\n",
    "print(all_data.dtypes, all_data.shape)\n",
    "print(all_data['random_sector'].value_counts())\n",
    "\n",
    "# 导入贝叶斯平滑类\n",
    "import numpy\n",
    "import random\n",
    "import scipy.special as special\n",
    "import math\n",
    "from math import log\n",
    "\n",
    "class HyperParam(object):\n",
    "    def __init__(self, alpha, beta): # 先初始化alpha和beta\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def sample_from_beta(self, alpha, beta, num, imp_upperbound):\n",
    "        sample = numpy.random.beta(alpha, beta, num)\n",
    "        I = []\n",
    "        C = []\n",
    "        for click_ratio in sample:\n",
    "            imp = random.random() * imp_upperbound\n",
    "            #imp = imp_upperbound\n",
    "            click = imp * click_ratio\n",
    "            I.append(imp)\n",
    "            C.append(click)\n",
    "        return I, C\n",
    "\n",
    "    # 更新方式1\n",
    "    def update_from_data_by_FPI(self, tries, success, iter_num, epsilon):\n",
    "        '''estimate alpha, beta using fixed point iteration(类似EM估计)\n",
    "        tries: 展示次数\n",
    "        success: 点击次数\n",
    "        iter_num: 迭代次数\n",
    "        epsilon: 精度\n",
    "        '''\n",
    "        for i in range(iter_num):\n",
    "            new_alpha, new_beta = self.__fixed_point_iteration(tries, success, self.alpha, self.beta)\n",
    "            # 当迭代稳定时，停止迭代\n",
    "            if abs(new_alpha - self.alpha) < epsilon and abs(new_beta - self.beta) < epsilon:\n",
    "                break\n",
    "            self.alpha = new_alpha\n",
    "            self.beta = new_beta\n",
    "\n",
    "    def __fixed_point_iteration(self, tries, success, alpha, beta):\n",
    "        '''fixed point iteration'''\n",
    "        sumfenzialpha = 0.0\n",
    "        sumfenzibeta = 0.0\n",
    "        sumfenmu = 0.0\n",
    "        for i in range(len(tries)):\n",
    "            sumfenzialpha += (special.digamma(success[i]+alpha) - special.digamma(alpha))\n",
    "            sumfenzibeta += (special.digamma(tries[i]-success[i]+beta) - special.digamma(beta))\n",
    "            sumfenmu += (special.digamma(tries[i]+alpha+beta) - special.digamma(alpha+beta))\n",
    "\n",
    "        return alpha*(sumfenzialpha/sumfenmu), beta*(sumfenzibeta/sumfenmu)\n",
    "\n",
    "    # 更新方式1\n",
    "    def update_from_data_by_moment(self, tries, success):\n",
    "        '''estimate alpha, beta using moment estimation(矩估计)\n",
    "        tries: 展示次数\n",
    "        success: 点击次数\n",
    "        '''\n",
    "        # 样本均值和样本方差\n",
    "        mean, var = self.__compute_moment(tries, success)\n",
    "        #print 'mean and variance: ', mean, var\n",
    "        #self.alpha = mean*(mean*(1-mean)/(var+0.000001)-1)\n",
    "        self.alpha = (mean+0.000001) * ((mean+0.000001) * (1.000001 - mean) / (var+0.000001) - 1)\n",
    "        #self.beta = (1-mean)*(mean*(1-mean)/(var+0.000001)-1)\n",
    "        self.beta = (1.000001 - mean) * ((mean+0.000001) * (1.000001 - mean) / (var+0.000001) - 1)\n",
    "\n",
    "    def __compute_moment(self, tries, success):\n",
    "        '''moment estimation(求样本均值和样本方差)'''\n",
    "        ctr_list = []\n",
    "        var = 0.0\n",
    "        for i in range(len(tries)):\n",
    "            ctr_list.append(float(success[i])/tries[i])\n",
    "        mean = sum(ctr_list)/len(ctr_list)\n",
    "        for ctr in ctr_list:\n",
    "            var += pow(ctr-mean, 2)\n",
    "\n",
    "        return mean, var/(len(ctr_list)-1)\n",
    "\n",
    "\n",
    "# 计算query、title的转换率 \n",
    "since = time.time()\n",
    "sec_size = 5\n",
    "frac_size = 0.5 \n",
    "convert_feature = ['query', 'title']\n",
    "for index, feature in enumerate(convert_feature):\n",
    "    print('正在计算' + feature + '转换率')\n",
    "    for sec in range(sec_size + 1):  # 0, 1, 2, 3, 4, 5  #0 is test， 1 is valid\n",
    "        print(sec, '折')\n",
    "        if sec == 1:\n",
    "            temp = all_data[(all_data.label != -1)&(all_data.random_sector != sec)][[feature, 'label']]\n",
    "        elif sec == 0:\n",
    "            temp = all_data[(all_data.label != -1)&(all_data.random_sector != sec)][[feature, 'label']]\n",
    "        else:\n",
    "            temp = all_data[(all_data.label != -1)&(all_data.random_sector != sec)&(all_data.random_sector != 1)][[feature, 'label']]\n",
    "            temp = temp.sample(frac = frac_size, random_state = 2019).reset_index(drop = True)\n",
    "        \n",
    "        temp[feature + '_all_count'] = temp.groupby(feature).label.transform('count')\n",
    "        temp[feature + '_label_count'] = temp.groupby(feature).label.transform('sum')\n",
    "\t\t# 贝叶斯平滑\n",
    "        HP = HyperParam(1, 1)\n",
    "        HP.update_from_data_by_moment(temp[feature + '_all_count'].values, temp[feature + '_label_count'].values)\n",
    "        temp[feature + '_convert'] = (temp[feature + '_label_count'] + HP.alpha) / (temp[feature + '_all_count'] + HP.alpha + HP.beta)\n",
    "        print('temp before shape:', temp.shape)\n",
    "        temp = temp[[feature, feature + '_convert']].drop_duplicates()\n",
    "        print('temp after shape:', temp.shape)\n",
    "\n",
    "        sec_data = all_data[all_data.random_sector == sec][[feature]]\n",
    "        all_data.loc[all_data.random_sector == sec, feature + '_convert'] = pd.merge(sec_data, temp, on=feature,\n",
    "                                                                                     how='left')[feature+'_convert'].values\n",
    "        del temp, sec_data\n",
    "        gc.collect()\n",
    "    all_data[feature + '_convert'] = all_data[feature + '_convert'].astype(np.float32)\n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "E4 = 400000000\n",
    "print(all_data[['label', 'query_convert', 'title_convert']][:E4].count())\n",
    "print(all_data[['label', 'query_convert', 'title_convert']][E4:E4+20000000].count())\n",
    "print(all_data[['label', 'query_convert', 'title_convert']][E4+20000000:].count())\n",
    "\n",
    "print('==========计算交叉转换率==========')\n",
    "since = time.time()\n",
    "sec_size = 5 \n",
    "frac_size = 0.5 \n",
    "convert_feature = ['query', 'title']\n",
    "first_feature, second_feature = convert_feature[0], convert_feature[1]\n",
    "print('正在计算' + first_feature + '和' + second_feature + '的转换率')\n",
    "for sec in range(sec_size + 1):\n",
    "    print(sec, '折')\n",
    "    if sec == 1:\n",
    "        temp = all_data[(all_data.label != -1)&(all_data.random_sector != sec)][[first_feature, second_feature, 'label']]\n",
    "    elif sec == 0:\n",
    "        temp = all_data[(all_data.label != -1)&(all_data.random_sector != sec)][[first_feature, second_feature, 'label']]\n",
    "    else:\n",
    "        temp = all_data[(all_data.label != -1)&(all_data.random_sector != sec)&(all_data.random_sector != 1)][[first_feature, second_feature, 'label']]\n",
    "        temp = temp.sample(frac = frac_size, random_state = 2019).reset_index(drop = True)\n",
    "\n",
    "    temp['query_title_all_count'] = temp.groupby([first_feature, second_feature]).label.transform('count')\n",
    "    temp['query_title_label_count'] = temp.groupby([first_feature, second_feature]).label.transform('sum')\n",
    "    \n",
    "    HP = HyperParam(1, 1)\n",
    "    HP.update_from_data_by_moment(temp['query_title_all_count'].values, temp['query_title_label_count'].values)\n",
    "    temp['query_title_convert']=(temp['query_title_label_count']+HP.alpha)/(temp['query_title_all_count']+HP.alpha+HP.beta)\n",
    "    \n",
    "    temp = temp[[first_feature, second_feature, 'query_title_convert']].drop_duplicates()\n",
    "    sec_data = all_data[all_data.random_sector == sec][[first_feature, second_feature]]\n",
    "    all_data.loc[all_data.random_sector == sec, 'query_title_convert']=pd.merge(sec_data, temp, on=[first_feature, second_feature],\n",
    "                                                                                how='left')['query_title_convert'].values\n",
    "    del temp, sec_data\n",
    "    gc.collect()\n",
    "all_data['query_title_convert'] = all_data['query_title_convert'].astype(np.float32)\n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "E4 = 400000000\n",
    "print(all_data[['label', 'query_convert', 'title_convert', 'query_title_convert']][:E4].count())\n",
    "print(all_data[['label', 'query_convert', 'title_convert', 'query_title_convert']][E4:E4+20000000].count())\n",
    "print(all_data[['label', 'query_convert', 'title_convert', 'query_title_convert']][E4+20000000:].count())\n",
    "\n",
    "\n",
    "# 保存cvr特征\n",
    "all_data[:E4][['query_convert', 'title_convert', \n",
    "               'query_title_convert']].to_csv('feature_4e/train4e_cvr.csv.gz', compression='gzip', index=False)\n",
    "all_data[E4:E4+20000000][['query_convert', 'title_convert', \n",
    "            'query_title_convert']].to_csv('feature_4e/testA_cvr.csv.gz', compression='gzip', index=False)\n",
    "all_data[E4+20000000:][['query_convert', 'title_convert', \n",
    "        'query_title_convert']].to_csv('feature_4e/testB_cvr.csv.gz', compression='gzip', index=False)\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BD66AE6000843468F21725C2D7B82FD"
   },
   "source": [
    "## 1.9 word2vec相似度\n",
    "特征9: 提取前4亿和最终两个测试集的word2vec句向量距离特征:'w2v_avg_cosine', 'w2v_avg_cityblock', 'w2v_avg_minkowski', 'w2v_avg_braycurtis', 'w2v_avg_canberra'\n",
    "\n",
    "* 实际将前两个作为特征9_0后三个作为特征9_1同时提取, 以下代码是9_0部分\n",
    "* 需提前训练好word2vec模型\n",
    "* 下面是用的100维的word2vec，实际还是用了300维的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "37007C4B3EB3455280C9B2CC5DBF8BC7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import paired_distances\n",
    "from scipy.spatial import distance\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "E4 = 400000000\n",
    "# TRAIN_ROWS = 1000000000\n",
    "TEST_A_ROWS, TEST_B_ROWS = 20000000, 100000000\n",
    "\n",
    "save_dir = \"/home/kesci/work/feature_4e\"\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "test_A_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "test_B_file_path = '/home/kesci/input/bytedance/bytedance_contest.final_2.csv'\n",
    "\n",
    "\n",
    "train_columns = ['query_id','query','query_title_id','title','label']\n",
    "test_columns = ['query_id','query','query_title_id','title']\n",
    "assert os.path.exists(save_dir)\n",
    "print(\"当前进程PID:\", os.getpid(), \"开始时间:\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "\n",
    "w2v_model_path = '/home/kesci/work/word2vec/w2v_100_cbow_4e.model'\n",
    "\n",
    "\n",
    "debug=False\n",
    "\n",
    "\n",
    "print(\"----- 从%s加载word2vec模型 -----\" % w2v_model_path)\n",
    "# 这里放上训练时的EpochSaver, 否则load会报错\n",
    "class EpochSaver(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    '''用于保存模型, 打印损失函数等等'''\n",
    "    def __init__(self, savedir, save_name=\"word2vector.model\"):\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        self.save_path = os.path.join(savedir, save_name)\n",
    "        self.epoch = 0\n",
    "        self.pre_loss = 0\n",
    "        self.best_loss = 999999999.9\n",
    "        self.since = time.time()\n",
    "        \n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        cum_loss = model.get_latest_training_loss() # 返回的是从第一个epoch累计的\n",
    "        epoch_loss = cum_loss - self.pre_loss\n",
    "        time_taken = time.time() - self.since\n",
    "        print(\"Epoch %d, loss: %.2f, time: %dmin %ds\" % \n",
    "                    (self.epoch, epoch_loss, time_taken//60, time_taken%60))\n",
    "        if self.best_loss > epoch_loss:\n",
    "            self.best_loss = epoch_loss\n",
    "            print(\"Better model. Best loss: %.2f\" % self.best_loss)\n",
    "            model.save(self.save_path)\n",
    "            print(\"Model %s save done!\" % self.save_path)\n",
    "            \n",
    "        self.pre_loss = cum_loss\n",
    "        self.since = time.time()\n",
    "        print(\"\\n\\n\\n\")\n",
    "w2v_model = Word2Vec.load(w2v_model_path)\n",
    "embed_size = w2v_model.trainables.layer1_size\n",
    "print(\"加载完毕. 总词典数: %d, embeding size: %d\" % (len(w2v_model.wv.vocab), embed_size))\n",
    "\n",
    "\n",
    "def w2v_sent2vec(sentence, model):\n",
    "    \"\"\"计算句子的平均word2vec向量, sentences是一个句子, 句向量最后会归一化\"\"\"\n",
    "    M = []\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            M.append(model.wv[word])\n",
    "        except KeyError: # 不在词典里\n",
    "            continue\n",
    "    if len(M) == 0:\n",
    "        return ((-1 / np.sqrt(embed_size)) * np.ones(embed_size)).astype(np.float32)\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return (v / np.sqrt((v ** 2).sum())).astype(np.float32)\n",
    "\n",
    "print(\" ----- 开始提取均word2vec相似度特征 -----\")\n",
    "since = time.time()\n",
    "fea_names = [\n",
    "    \"w2v_avg_cosine\",\n",
    "    \"w2v_avg_cityblock\",\n",
    "    # \"w2v_avg_minkowski\",\n",
    "    # \"w2v_avg_braycurtis\",\n",
    "    # \"w2v_avg_canberra\"\n",
    "    ]\n",
    "name2idx = {fea_name:fea_names.index(fea_name) for fea_name in fea_names}\n",
    "print(\"一共%d个特征:\" % len(fea_names))\n",
    "print(fea_names, flush=True)\n",
    "\n",
    "N = (300000 if debug else (E4 + TEST_A_ROWS + TEST_B_ROWS))\n",
    "all_data = np.zeros((N, len(fea_names)), dtype=np.float32)\n",
    "\n",
    "print(\"train:\", flush=True)\n",
    "since = time.time()\n",
    "with open(train_file_path) as train_csvfile:\n",
    "    csv_reader = csv.reader(train_csvfile)\n",
    "    for index, line in tqdm(enumerate(csv_reader)):\n",
    "        query = line[1].strip()\n",
    "        title = line[3].strip()\n",
    "        \n",
    "        query_avg_w2v = w2v_sent2vec(query, w2v_model)\n",
    "        title_avg_w2v = w2v_sent2vec(title, w2v_model)\n",
    "        \n",
    "        all_data[index, name2idx[\"w2v_avg_cosine\"]] = distance.cosine(query_avg_w2v, title_avg_w2v)\n",
    "        all_data[index, name2idx[\"w2v_avg_cityblock\"]] = distance.cityblock(query_avg_w2v, title_avg_w2v)\n",
    "        # all_data[index, name2idx[\"w2v_avg_minkowski\"]] = distance.minkowski(query_avg_w2v, title_avg_w2v)\n",
    "        # all_data[index, name2idx[\"w2v_avg_braycurtis\"]] = distance.braycurtis(query_avg_w2v, title_avg_w2v)\n",
    "        # all_data[index, name2idx[\"w2v_avg_canberra\"]] = distance.canberra(query_avg_w2v, title_avg_w2v)\n",
    "        \n",
    "        if index == (E4 - 1):\n",
    "            print(\"训练集前4亿提取完成!\", flush=True)\n",
    "            break\n",
    "        if debug and index == 100000 - 1:\n",
    "            print(\"debug mode. break\")\n",
    "            break\n",
    "        \n",
    "time_elapsed = time.time() - since\n",
    "print('complete with idx %d in %d min %d s.' % (index, time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "\n",
    "offset = index + 1\n",
    "print(\"test A (offset=%d)...\" % offset, flush=True)\n",
    "since = time.time()\n",
    "with open(test_A_file_path) as test_A_csvfile:\n",
    "    csv_reader = csv.reader(test_A_csvfile)\n",
    "    for index, line in enumerate(csv_reader):\n",
    "        \n",
    "        query = line[1].strip()\n",
    "        title = line[3].strip()\n",
    "        \n",
    "        query_avg_w2v = w2v_sent2vec(query, w2v_model)\n",
    "        title_avg_w2v = w2v_sent2vec(title, w2v_model)\n",
    "        \n",
    "        all_data[index + offset, name2idx[\"w2v_avg_cosine\"]] = distance.cosine(query_avg_w2v, title_avg_w2v)\n",
    "        all_data[index + offset, name2idx[\"w2v_avg_cityblock\"]] = distance.cityblock(query_avg_w2v, title_avg_w2v)\n",
    "        # all_data[index + offset, name2idx[\"w2v_avg_minkowski\"]] = distance.minkowski(query_avg_w2v, title_avg_w2v)\n",
    "        # all_data[index + offset, name2idx[\"w2v_avg_braycurtis\"]] = distance.braycurtis(query_avg_w2v, title_avg_w2v)\n",
    "        # all_data[index + offset, name2idx[\"w2v_avg_canberra\"]] = distance.canberra(query_avg_w2v, title_avg_w2v)\n",
    "        \n",
    "        if debug and index == 100000 - 1:\n",
    "            print(\"debug mode. break\")\n",
    "            break\n",
    "time_elapsed = time.time() - since\n",
    "print('complete with idx %d in %d min %d s.' % (index, time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "\n",
    "offset = offset + index + 1\n",
    "print(\"test B (offset=%d)...\" % offset, flush=True)\n",
    "since = time.time()\n",
    "with open(test_B_file_path) as test_B_csvfile:\n",
    "    csv_reader = csv.reader(test_B_csvfile)\n",
    "    for index, line in enumerate(csv_reader):\n",
    "        \n",
    "        query = line[1].strip()\n",
    "        title = line[3].strip()\n",
    "        \n",
    "        query_avg_w2v = w2v_sent2vec(query, w2v_model)\n",
    "        title_avg_w2v = w2v_sent2vec(title, w2v_model)\n",
    "        \n",
    "        all_data[index + offset, name2idx[\"w2v_avg_cosine\"]] = distance.cosine(query_avg_w2v, title_avg_w2v)\n",
    "        all_data[index + offset, name2idx[\"w2v_avg_cityblock\"]] = distance.cityblock(query_avg_w2v, title_avg_w2v)\n",
    "        # all_data[index + offset, name2idx[\"w2v_avg_minkowski\"]] = distance.minkowski(query_avg_w2v, title_avg_w2v)\n",
    "        # all_data[index + offset, name2idx[\"w2v_avg_braycurtis\"]] = distance.braycurtis(query_avg_w2v, title_avg_w2v)\n",
    "        # all_data[index + offset, name2idx[\"w2v_avg_canberra\"]] = distance.canberra(query_avg_w2v, title_avg_w2v)\n",
    "        \n",
    "        if debug and index == 100000 - 1:\n",
    "            print(\"debug mode. break\")\n",
    "            break\n",
    "time_elapsed = time.time() - since\n",
    "print('complete with idx %d in %d min %d s.' % (index, time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "print(all_data.shape)\n",
    "print(all_data[:5, :])\n",
    "print(all_data[-5:, :])\n",
    "print(all_data.shape)\n",
    "\n",
    "if not debug:\n",
    "    train_save_path = os.path.join(save_dir, 'feature_9_0_4e.csv.gz')\n",
    "    print(\"保存4亿训练集到%s...\" % train_save_path)\n",
    "    np.savetxt(train_save_path, all_data[:E4, :], fmt=\"%f\", delimiter=\",\", \n",
    "                header=\",\".join(fea_names), comments=\"\")\n",
    "                \n",
    "    test_A_save_path = os.path.join(save_dir, 'feature_9_0_4e_testA.csv.gz')\n",
    "    print(\"保存测试集A到%s...\" % test_A_save_path)\n",
    "    np.savetxt(test_A_save_path, all_data[E4:E4+TEST_A_ROWS, :], fmt=\"%f\", delimiter=\",\", \n",
    "                header=\",\".join(fea_names), comments=\"\")\n",
    "    print(\"save done!\")\n",
    "    \n",
    "    test_B_save_path = os.path.join(save_dir, 'feature_9_0_4e_testB.csv.gz')\n",
    "    print(\"保存测试集B到%s...\" % test_B_save_path)\n",
    "    np.savetxt(test_B_save_path, all_data[E4+TEST_A_ROWS:, :], fmt=\"%f\", delimiter=\",\", \n",
    "                header=\",\".join(fea_names), comments=\"\")\n",
    "    print(\"save done!\")\n",
    "\n",
    "# del(all_data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C50E48FB3AAB4CF297FE581E930AE079"
   },
   "source": [
    "## 1.10 全局出现频次相关\n",
    "使用前4亿原始数据加上最后两个测试集计算特征: 特征: ['query_title_click', 'query_nunique_title', 'query_click', 'title_nunique_query', 'title_click']\n",
    "\n",
    "* query_nunique_title: 这个query对应多少unique的title\n",
    "* title_nunique_query\n",
    "* query_click: 所有数据中这个query出现的次数\n",
    "* title_click: 所有数据中这个title出现的次数\n",
    "* query_title_click: 所有数据中这个query+title出现的次数\n",
    "\n",
    "一下跑内存会炸, 分成两个2亿跑, 下面是后两亿\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "E2C4DFA4D6E84CA98B3DF8EEAB7C1631"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "    \n",
    "E4 = 400000000\n",
    "\n",
    "# TRAIN_ROWS = 1000000000\n",
    "TEST_A_ROWS, TEST_B_ROWS = 20000000, 100000000\n",
    "save_dir = \"/home/kesci/work/feature_4e\"\n",
    "assert os.path.exists(save_dir)\n",
    "\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "test_A_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "test_B_file_path = '/home/kesci/input/bytedance/bytedance_contest.final_2.csv'\n",
    "\n",
    "train_columns = ['query_id','query','query_title_id','title','label']\n",
    "test_columns = ['query_id','query','query_title_id','title']\n",
    "print(\"当前进程PID:\", os.getpid(), \"开始时间:\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "\n",
    "debug = False\n",
    "nrows = None\n",
    "if debug:\n",
    "    nrows = 1000000\n",
    "\n",
    "print(\"加载train 2e...\")\n",
    "train_df = pd.read_csv(train_file_path, names=train_columns, skiprows = (E4 // 2),\n",
    "                        nrows=(nrows if debug else (E4 // 2)))[['query','title']]\n",
    "print(train_df.shape)\n",
    "                        \n",
    "print('加载testA...')\n",
    "test_A_df = pd.read_csv(test_A_file_path, names=test_columns, nrows=nrows)[['query','title']]\n",
    "test_A_df[\"title\"] = test_A_df[\"title\"].apply(lambda x: x.strip()) # 去掉测试集title最后的tab\n",
    "print(test_A_df.shape)\n",
    "\n",
    "\n",
    "print('加载testB...')\n",
    "test_B_df = pd.read_csv(test_B_file_path, names=test_columns, nrows=nrows)[['query','title']]\n",
    "test_B_df[\"title\"] = test_B_df[\"title\"].apply(lambda x: x.strip()) # 去掉测试集title最后的tab\n",
    "print(test_B_df.shape)\n",
    "\n",
    "\n",
    "all_data=pd.concat((train_df, test_A_df, test_B_df), axis = 0, ignore_index = True, sort = False, copy=False)\n",
    "del train_df, test_A_df, test_B_df\n",
    "gc.collect()\n",
    "print('finish concat data')\n",
    "\n",
    "print(all_data.info())\n",
    "print(all_data.head())\n",
    "\n",
    "\n",
    "print(\"------------------- 提取 nunique 特征 ----------------------\")\n",
    "since = time.time()\n",
    "print(\"-- query_title_click...\")\n",
    "all_data['query_title_click'] = all_data.groupby(['query', 'title']).query.transform('count').astype(\"int32\")\n",
    "\n",
    "print(\"-- group by query:\")\n",
    "print(\"---- group...\")\n",
    "query_gb = all_data.groupby('query').title\n",
    "print(\"---- query_nunique_title...\")\n",
    "all_data['query_nunique_title'] = query_gb.transform('nunique').astype(\"int32\")\n",
    "print(\"---- query_click...\")\n",
    "all_data['query_click'] = query_gb.transform('count').astype(\"int32\")\n",
    "del query_gb\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print(\"-- group by title:\")\n",
    "print(\"---- group...\")\n",
    "title_gb = all_data.groupby('title').query\n",
    "print(\"---- title_nunique_query...\")\n",
    "all_data['title_nunique_query'] = title_gb.transform('nunique').astype(\"int32\")\n",
    "print(\"---- title_click...\")\n",
    "all_data['title_click'] = title_gb.transform('count').astype(\"int32\")\n",
    "del title_gb\n",
    "gc.collect()\n",
    "\n",
    "print(all_data.head())\n",
    "print(all_data.tail())\n",
    "print(all_data.info())\n",
    "\n",
    "if not debug:\n",
    "    PART = E4 // 2\n",
    "    feature_names = [c for c in all_data.columns if c not in ['query', 'title']]\n",
    "    \n",
    "    train_save_path = os.path.join(save_dir, 'feature_10_1th_2e.csv.gz')\n",
    "    # 注意loc的索引是左闭右闭的!!!\n",
    "    print(\"保存训练集到%s...\" % train_save_path)\n",
    "    all_data.loc[:PART-1, feature_names].to_csv(train_save_path, compression='gzip', index=False)\n",
    "    \n",
    "    \n",
    "    test_A_save_path = os.path.join(save_dir, 'feature_10_1th_2e_testA.csv.gz')\n",
    "    print(\"保存测试集A到%s...\" % test_A_save_path)\n",
    "    all_data.loc[PART:PART+TEST_A_ROWS - 1, feature_names].to_csv(test_A_save_path, compression='gzip', index=False)\n",
    "    \n",
    "    test_B_save_path = os.path.join(save_dir, 'feature_10_1th_2e_testB.csv.gz')\n",
    "    print(\"保存测试集B到%s...\" % test_B_save_path)\n",
    "    all_data.loc[PART+TEST_A_ROWS:, feature_names].to_csv(test_B_save_path, compression='gzip', index=False)\n",
    "    \n",
    "    print(\"save done!\")\n",
    "    del all_data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A546616E538444BD868371ADC4230CE4",
    "mdEditEnable": false
   },
   "source": [
    "## 1.11 hc 补充特征 (4e_train + 2kw_test + 1e_test_final) 8 个特征\n",
    "(jaccard_similarity,\n",
    "qt_coword_query_ratio,\n",
    "qt_coword_title_ratio,\n",
    "qt_len_mean,\n",
    "qt_common_word_acc,\n",
    "ngram_query_title_precision,\n",
    "ngram_query_title_recall,\n",
    "ngram_query_title_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "738A821276014CFDAED12076673376B4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm as tqdm \n",
    "import csv, json\n",
    "import os, gc, time, math\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "save_dir = \"/home/kesci/work/feature\"\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "testA_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "testB_file_path = '/home/kesci/input/bytedance/bytedance_contest.final_2.csv'\n",
    "assert os.path.exists(save_dir)\n",
    "TRAIN_ROWS, TESTA_ROWS, TESTB_ROWS=400000000, 20000000, 100000000\n",
    "train_columns = ['query_id', 'query', 'query_title_id', 'title', 'label']\n",
    "test_columns = ['query_id', 'query', 'query_title_id', 'title']\n",
    "\n",
    "# ----- 提取特征 -----\n",
    "since = time.time()\n",
    "fea_names = [\"jaccard_similarity\", \"qt_coword_query_ratio\", \"qt_coword_title_ratio\", \n",
    "             \"qt_len_mean\", \"qt_common_word_acc\", \n",
    "             \"ngram_query_title_precision\", \"ngram_query_title_recall\", \"ngram_query_title_acc\"]\n",
    "print(\"一共%d个特征:\" % len(fea_names), fea_names)\n",
    "name2idx = {fea_name:fea_names.index(fea_name) for fea_name in fea_names}\n",
    "\n",
    "# 训练集4e \n",
    "def get_ngram_rp_query_in_title(query, title):\n",
    "    query = list(query.strip().split())\n",
    "    title = list(title.strip().split())\n",
    "    query_2gram = []\n",
    "    for i in range(len(query) - 1):\n",
    "        query_2gram.append(query[i]+query[i+1])\n",
    "    query.extend(query_2gram)\n",
    "    \n",
    "    title_2gram = []\n",
    "    for i in range(len(title) - 1):\n",
    "        title_2gram.append(title[i]+title[i+1])\n",
    "    title.extend(title_2gram)\n",
    "    \n",
    "    len_query = len(query)\n",
    "    len_title = len(title)\n",
    "    len_common = len(set(query) & set(title))\n",
    "    \n",
    "    recall = len_common / (len_query + 0.001)\n",
    "    precision = len_common / (len_title + 0.001)\n",
    "    acc = len_common / (len_query + len_title - len_common)\n",
    "    return [recall, precision, acc]\n",
    "\n",
    "with open(train_file_path) as train_csvfile:\n",
    "    for idx, line in tqdm(enumerate(csv.reader(train_csvfile))):\n",
    "        query_set = set(line[1].strip().split())\n",
    "        title_set = set(line[3].strip().split())\n",
    "        common_words_len = len(query_set&title_set)\n",
    "        query_len = len(line[1].strip().split())\n",
    "        title_len = len(line[3].strip().split())\n",
    "        recall, precision, acc = get_ngram_rp_query_in_title(line[1], line[3])\n",
    "        \n",
    "        all_data[idx, name2idx[\"jaccard_similarity\"]] = common_words_len/len(query_set|title_set)\n",
    "        all_data[idx, name2idx[\"qt_coword_query_ratio\"]] = common_words_len/query_len\n",
    "        all_data[idx, name2idx[\"qt_coword_title_ratio\"]] = common_words_len/title_len\n",
    "        all_data[idx, name2idx[\"qt_len_mean\"]] = (query_len + title_len)/2.0\n",
    "        all_data[idx, name2idx[\"qt_common_word_acc\"]]=common_words_len/(query_len+title_len-common_words_len)\n",
    "        all_data[idx, name2idx[\"ngram_query_title_precision\"]] = precision\n",
    "        all_data[idx, name2idx[\"ngram_query_title_recall\"]] = recall\n",
    "        all_data[idx, name2idx[\"ngram_query_title_acc\"]]= acc\n",
    "        if idx == TRAIN_ROWS-1: break\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete with idx %d in %d min %d s.' % (idx, time_elapsed // 60, time_elapsed % 60))\n",
    "print(\"saving...\")\n",
    "save_path = os.path.join(save_dir, 'feature_train_4e_hc_add.csv.gz')\n",
    "np.savetxt(save_path, all_data, fmt=\"%f\", delimiter=\",\", header=\",\".join(fea_names), comments=\"\")\n",
    "print(save_path, \"save done!\")\n",
    "del (all_data)\n",
    "gc.collect()\n",
    "\n",
    "# 测试集2kw\n",
    "all_data = np.zeros((TESTA_ROWS, len(fea_names)), dtype=np.float32)\n",
    "with open(testA_file_path) as test_csvfile:\n",
    "    for idx, line in tqdm(enumerate(csv.reader(test_csvfile))):\n",
    "        query_set = set(line[1].strip().split())\n",
    "        title_set = set(line[3].strip().split())\n",
    "        common_words_len = len(query_set&title_set)\n",
    "        query_len = len(line[1].strip().split())\n",
    "        title_len = len(line[3].strip().split())\n",
    "        recall, precision, acc = get_ngram_rp_query_in_title(line[1], line[3])\n",
    "        \n",
    "        all_data[idx, name2idx[\"jaccard_similarity\"]] = common_words_len/len(query_set|title_set)\n",
    "        all_data[idx, name2idx[\"qt_coword_query_ratio\"]] = common_words_len/query_len\n",
    "        all_data[idx, name2idx[\"qt_coword_title_ratio\"]] = common_words_len/title_len\n",
    "        all_data[idx, name2idx[\"qt_len_mean\"]] = (query_len + title_len)/2.0\n",
    "        all_data[idx, name2idx[\"qt_common_word_acc\"]] = common_words_len/(query_len+title_len-common_words_len)\n",
    "        \n",
    "        all_data[idx, name2idx[\"ngram_query_title_precision\"]] = precision\n",
    "        all_data[idx, name2idx[\"ngram_query_title_recall\"]] = recall\n",
    "        all_data[idx, name2idx[\"ngram_query_title_acc\"]]= acc\n",
    "time_elapsed = time.time() - since\n",
    "print('complete with idx %d in %d min %d s.' % (idx, time_elapsed // 60, time_elapsed % 60))\n",
    "print(\"saving...\")\n",
    "save_path = os.path.join(save_dir, 'feature_test_2kw_hc_add.csv.gz')\n",
    "np.savetxt(save_path, all_data, fmt=\"%f\", delimiter=\",\", header=\",\".join(fea_names), comments=\"\")\n",
    "print(save_path, \"save done!\")\n",
    "del (all_data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "129B89FCFF9E4023AA5888EEF0D89789"
   },
   "source": [
    "# 2 word2vec模型训练\n",
    "语料库包括前4亿训练数据加上两个测试集, 下面使用300维的词向量（实际还训练了一个100d的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "7A80990B15AA4D15AEFDBCFB3ABD7CAF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import gensim\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(levelname)s-%(asctime)s:%(message)s\", datefmt='%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "E4 = 400000000\n",
    "# TRAIN_ROWS = 1000000000\n",
    "TEST_A_ROWS, TEST_B_ROWS = 20000000, 100000000\n",
    "\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "test_A_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "test_B_file_path = '/home/kesci/input/bytedance/bytedance_contest.final_2.csv'\n",
    "\n",
    "savedir = \"/home/kesci/work/word2vec\"\n",
    "save_name = \"w2v_300_cbow_4e.model\"\n",
    "\n",
    "vector_size=300\n",
    "\n",
    "debug=False\n",
    "nrows = None\n",
    "if debug:\n",
    "    nrows=5000000\n",
    "    \n",
    "print(\"当前进程PID:\", os.getpid(), \"开始时间:\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "\n",
    "def csv_generator(csv_file_path, have_header=False, num=None):\n",
    "    if num is None:\n",
    "        num = -1\n",
    "    with open(csv_file_path) as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        if have_header:\n",
    "            print(\"跳过表头!!!\")\n",
    "            next(csv_reader) # 跳过表头\n",
    "        count = 0\n",
    "        for line in csv_reader:\n",
    "            if count == num:\n",
    "                return \n",
    "            count += 1\n",
    "            yield line\n",
    "\n",
    "def query_generator(csv_file_path, query_idx=1, have_header=False, num=None):\n",
    "    for line in csv_generator(csv_file_path, have_header=have_header, num=num):\n",
    "        yield line[query_idx].strip().split() # strip去掉测试集里每个title最后的tab\n",
    "        \n",
    "def title_generator(csv_file_path, title_idx=3, have_header=False, num=None):\n",
    "    for line in csv_generator(csv_file_path, have_header=have_header, num=num):\n",
    "        yield line[title_idx].strip().split()\n",
    "        \n",
    "        \n",
    "def query_title_generator(train_nrows, test_nrows):\n",
    "    return chain(query_generator(train_file_path, have_header=False, num=train_nrows), \n",
    "                title_generator(train_file_path, have_header=False, num=train_nrows),\n",
    "                query_generator(test_A_file_path, have_header=False, num=test_nrows), \n",
    "                title_generator(test_A_file_path, have_header=False, num=test_nrows),\n",
    "                query_generator(test_B_file_path, have_header=False, num=test_nrows), \n",
    "                title_generator(test_B_file_path, have_header=False, num=test_nrows))\n",
    "\n",
    "class SentenceIterator(object):\n",
    "    \"\"\"语料库生成器, 包含整个训练集和测试集\"\"\"\n",
    "    def __init__(self, train_nrows, test_nrows):\n",
    "        self.train_nrows = train_nrows\n",
    "        self.test_nrows = test_nrows\n",
    "        if not debug:\n",
    "            assert test_nrows == None\n",
    "    def __iter__(self):\n",
    "        for sentence in query_title_generator(self.train_nrows, self.test_nrows):\n",
    "            yield sentence\n",
    "            \n",
    "class EpochSaver(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    '''用于保存模型, 打印损失函数等等'''\n",
    "    def __init__(self, savedir, save_name=\"word2vector.model\"):\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        self.save_path = os.path.join(savedir, save_name)\n",
    "        self.epoch = 0\n",
    "        self.pre_loss = 0\n",
    "        self.best_loss = 999999999.9\n",
    "        self.since = time.time()\n",
    "        \n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        cum_loss = model.get_latest_training_loss() # 返回的是从第一个epoch累计的\n",
    "        epoch_loss = cum_loss - self.pre_loss\n",
    "        time_taken = time.time() - self.since\n",
    "        print(\"Epoch %d, loss: %.2f, time: %dmin %ds\" % \n",
    "                    (self.epoch, epoch_loss, time_taken//60, time_taken%60))\n",
    "        if self.best_loss >= epoch_loss:\n",
    "            self.best_loss = epoch_loss\n",
    "            print(\"Better model. Best loss: %.2f\" % self.best_loss)\n",
    "            model.save(self.save_path)\n",
    "            print(\"Model %s save done!\" % self.save_path)\n",
    "\n",
    "        self.pre_loss = cum_loss\n",
    "        self.since = time.time()\n",
    "        print(\"\\n\\n\\n\")\n",
    "        \n",
    "        \n",
    "# 将整个过程分成三步\n",
    "# 1, 构建模型(不训练)\n",
    "model_word2vec = gensim.models.Word2Vec(min_count=5, \n",
    "                                        window=5, \n",
    "                                        sg=0, # cbow\n",
    "                                        size=vector_size,\n",
    "                                        workers=8,\n",
    "                                        seed=2019,\n",
    "                                        batch_words=100000)\n",
    "# 2, 遍历一遍语料库\n",
    "since = time.time()\n",
    "sentences = SentenceIterator((nrows if debug else E4), nrows)\n",
    "model_word2vec.build_vocab(sentences, progress_per=(500000 if debug else 50000000))\n",
    "time_elapsed = time.time() - since\n",
    "print('Time to build vocab: {:.0f}min {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "# 3, 训练\n",
    "model_word2vec.train(sentences, total_examples=model_word2vec.corpus_count, \n",
    "                        epochs=40, compute_loss=True, \n",
    "                        report_delay=(10 if debug else 60*10), # 每隔10分钟输出一下日志\n",
    "                        callbacks=[EpochSaver(savedir, save_name)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FA57F4096DD43B28C24B00438258C14"
   },
   "source": [
    "# 3 lgb\n",
    "## 3.1 训练\n",
    "* 使用的特征包括前面提到的10种特征加上100维的的word2vec平均句向量直接作差特征。\n",
    "* 四千万跑一次（后20%作为验证），4亿数据一共跑10次\n",
    "* 注意特征10是分成了两个2亿，所以前两亿和后两亿的代码略有不同，下面的代码是前2亿的lgb训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "9DFB0C164745441DB7DB59403769D651"
   },
   "outputs": [],
   "source": [
    "# lightgbm train and predict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics.pairwise import paired_distances\n",
    "from sklearn import metrics\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models import Word2Vec\n",
    "# from glove import Glove\n",
    "import pickle\n",
    "# from sklearn.externals import joblib\n",
    "import os\n",
    "import csv\n",
    "import psutil\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "import math\n",
    "\n",
    "if not os.path.isdir('lgb_model_result'):\n",
    "    os.mkdir('lgb_model_result')\n",
    "\n",
    "TEST_A_ROWS, TEST_B_ROWS = 20000000, 100000000\n",
    "E2 = 200000000\n",
    "chunksize=40000000 #每次4kw训练数据\n",
    "assert E2 % chunksize == 0\n",
    "print(\"只用前4亿训练数据!\")\n",
    "iter_num = int(E2 // chunksize)\n",
    "\n",
    "debug = False\n",
    "if debug:\n",
    "    chunksize = 1000000\n",
    "    iter_num = 2\n",
    "\n",
    "\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "test_A_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "test_B_file_path = '/home/kesci/input/bytedance/bytedance_contest.final_2.csv'\n",
    "\n",
    "train_columns = ['query_id','query','query_title_id','title','label']\n",
    "test_columns = ['query_id','query','query_title_id','title']\n",
    "print(\"当前进程PID:\", os.getpid(), \"开始时间:\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "\n",
    "\n",
    "# 这个文件存放了训练集label\n",
    "train_label_file = \"/home/kesci/work/feature/train_label.csv.gz\"\n",
    "save_dir = \"/home/kesci/work/feature\"\n",
    "\n",
    "# 之前提取好的训练集特征文件\n",
    "files = [\n",
    "    \"/home/kesci/work/feature/feature_1_0.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_2.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_3.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_4.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_5.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_6.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_7.csv.gz\",\n",
    "    \n",
    "    \"/home/kesci/work/feature_4e/feature_1_1_4e.csv.gz\",\n",
    "    \"/home/kesci/work/feature_4e/feature_9_0_4e.csv.gz\",\n",
    "    \"/home/kesci/work/feature_4e/feature_9_1_4e.csv.gz\",\n",
    "    \"/home/kesci/work/feature_4e/feature_9_0_300d_4e.csv.gz\",\n",
    "    \"/home/kesci/work/feature_4e/feature_9_1_300d_4e.csv.gz\",\n",
    "    \n",
    "    # cvr\n",
    "    \"/home/kesci/work/feature_4e/feature_8_4e.csv.gz\",\n",
    " \n",
    "    \"/home/kesci/work/feature_4e/feature_10_0th_2e.csv.gz\", # 0-2亿\n",
    "    # \"/home/kesci/work/feature_4e/feature_10_1th_2e.csv.gz\", # 2-4亿\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "used_features = [\n",
    "# feature 1_0\n",
    "'query_length','title_length','WordMatchShare','WordMatchShare_query',\n",
    "'WordMatchShare_title',\n",
    "'LengthDiff', 'LengthDiffRate', 'LengthRatio_qt', 'LengthRatio_tq', # 这四个根据前面的计算得到\n",
    "\n",
    "# feature 1_1\n",
    "'TFIDFWordMatchShare','TFIDFWordMatchShare_query','TFIDFWordMatchShare_title',\n",
    "\n",
    "\n",
    "# feature 2\n",
    "'NgramJaccardCoef_1', 'NgramJaccardCoef_2', 'NgramJaccardCoef_3', 'NgramJaccardCoef_4',\n",
    "\n",
    "# feature 3\n",
    "'Levenshtein_ratio', 'Levenshtein_distance_char','query_title_common_words', 'common_word_ratio',\n",
    "\n",
    "# feature 4\n",
    "'lcsubstr_len', 'lcseque_len','longest_match_size', 'longest_match_ratio',\n",
    "\n",
    "# feature 5\n",
    "'fuzz_qratio', 'fuzz_partial_ratio',\n",
    "\n",
    "#feature 6\n",
    "'fuzz_partial_token_sort_ratio','fuzz_token_set_ratio','fuzz_token_sort_ratio',\n",
    "\n",
    "#feature 7\n",
    "'query_Entropy','title_Entropy','query_title_Entropy','WordMatchShare_Entropy',\n",
    "\n",
    "#feature 8\n",
    "\"query_convert\", \"title_convert\", \"query_title_convert\",\n",
    "\n",
    "# fature 9_0\n",
    "'w2v_avg_cosine', 'w2v_avg_cityblock',\n",
    "\n",
    "# feature 9_1\n",
    "'w2v_avg_minkowski', 'w2v_avg_braycurtis', 'w2v_avg_canberra',\n",
    "\n",
    "# fature 9_0 300d\n",
    "'w2v300_avg_cosine', 'w2v300_avg_cityblock',\n",
    "\n",
    "# feature 9_1 300d\n",
    "'w2v300_avg_minkowski', 'w2v300_avg_braycurtis', 'w2v300_avg_canberra',\n",
    "\n",
    "# feature 10\n",
    "'query_title_click', 'query_nunique_title', 'query_click', 'title_nunique_query', 'title_click',\n",
    "\n",
    "]\n",
    "\n",
    "# gensim feature\n",
    "class EpochSaver(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    '''用于保存模型, 打印损失函数等等'''\n",
    "    def __init__(self, savedir, save_name=\"word2vector.model\"):\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        self.save_path = os.path.join(savedir, save_name)\n",
    "        self.epoch = 0\n",
    "        self.pre_loss = 0\n",
    "        self.best_loss = 999999999.9\n",
    "        self.since = time.time()\n",
    "        \n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        cum_loss = model.get_latest_training_loss() # 返回的是从第一个epoch累计的\n",
    "        epoch_loss = cum_loss - self.pre_loss\n",
    "        time_taken = time.time() - self.since\n",
    "        print(\"Epoch %d, loss: %.2f, time: %dmin %ds\" % \n",
    "                    (self.epoch, epoch_loss, time_taken//60, time_taken%60))\n",
    "        if self.best_loss > epoch_loss:\n",
    "            self.best_loss = epoch_loss\n",
    "            print(\"Better model. Best loss: %.2f\" % self.best_loss)\n",
    "            model.save(self.save_path)\n",
    "            print(\"Model %s save done!\" % self.save_path)\n",
    "            \n",
    "        self.pre_loss = cum_loss\n",
    "        self.since = time.time()\n",
    "        print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "def query_title_generator(csv_file_path, skiprows=0, have_header=False, max_rows=None):\n",
    "    if max_rows is None:\n",
    "        max_rows = -1\n",
    "    with open(csv_file_path) as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        if have_header:\n",
    "            print(\"跳过表头!!!\")\n",
    "            next(csv_reader) # 跳过表头\n",
    "        for _ in range(skiprows):\n",
    "            next(csv_reader)\n",
    "        \n",
    "        count = 0\n",
    "        for line in csv_reader:\n",
    "            if count == max_rows:\n",
    "                return\n",
    "            count += 1\n",
    "            yield line[1].strip().split(), line[3].strip().split() # query, title\n",
    "\n",
    "def w2v_sent2vec(words, model):\n",
    "    \"\"\"计算句子的平均word2vec向量, sentences是一个句子, 句向量最后会归一化\"\"\"\n",
    "    M = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            M.append(model.wv[word])\n",
    "        except KeyError: # 不在词典里\n",
    "            continue\n",
    "    \n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return (v / np.sqrt((v ** 2).sum())).astype(np.float32)\n",
    "\n",
    "\n",
    "lgb_params = {\n",
    "    \"objective\": 'binary',\n",
    "    \"num_leaves\": 128, # 32, # 128, # 30, # 50, #64, \n",
    "    \"max_depth\": -1, #7, #7, #7, # 5, #-1,\n",
    "    \"learning_rate\": 0.05, # 0.02, \n",
    "    # \"n_estimators\": 100000,\n",
    "#   \"subsample_for_bin\"=200000, \n",
    "    \"min_child_samples\": 20,\n",
    "    \"min_child_weight\": 0.001, \n",
    "    \"min_split_gain\": 0.0, \n",
    "            \n",
    "    \"subsample\": 0.7,#0.5,    # 构造每棵树使用的样本率\n",
    "    \"subsample_freq\": 1, \n",
    "    \"colsample_bytree\": 0.7, # 构造每棵树使用的特征率\n",
    "            \n",
    "    \"reg_alpha\": 2,# 5, #0, #10, \n",
    "    \"reg_lambda\": 2,# 5, # 0, #10,          \n",
    "\n",
    "    \"n_jobs\": 12, # -1, \n",
    "    \"random_state\": 2018\n",
    "}\n",
    "print(\"n jobs:\", lgb_params[\"n_jobs\"])\n",
    "print(\"lgb学习率: \", lgb_params[\"learning_rate\"])\n",
    "print(\"lgb num_leaves: \",  lgb_params[\"num_leaves\"])\n",
    "print(\"lgb max_depth: \",  lgb_params[\"max_depth\"])\n",
    "\n",
    "def load_feature_csv(csv_file, start_row=0, nrows=chunksize, drop=True):\n",
    "    \"\"\"如果是读取特征文件那么drop=True以丢弃不用的feature, 否则应该设成False, 例如读id文件\"\"\"\n",
    "    since = time.time()\n",
    "    base_feature_df = pd.read_csv(csv_file, dtype=\"float32\", names=None, \n",
    "                                skiprows=start_row, nrows=nrows)\n",
    "    df_len = base_feature_df.shape[0]\n",
    "    print('load %d rows from %s(start from row %d)' \\\n",
    "                        % (df_len, csv_file.split(\"/\")[-1], start_row), end=\" \")\n",
    "                        \n",
    "    columns = pd.read_csv(csv_file, dtype=\"float32\", nrows=1).columns\n",
    "    base_feature_df.columns = columns\n",
    "    print(columns.values)    \n",
    "    unused_features = [fea for fea in columns if fea not in used_features]\n",
    "    if drop and len(unused_features) != 0:\n",
    "        print(\"\\n丢弃掉feature:\", unused_features, end=\"\")\n",
    "        base_feature_df = base_feature_df.drop(unused_features, axis=1)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n",
    "    return base_feature_df\n",
    "    \n",
    "\n",
    "# ---- 按顺序记录特征名(必须和后面读取特征的顺序保持一致！！！) ------\n",
    "feature_names = []\n",
    "# word2vec 直接作差特征\n",
    "feature_names.extend([\"w2v_100_diff_%d\"%i for i in range(100)])\n",
    "print(\"特征文件:\")\n",
    "for f in files:\n",
    "    print(f)\n",
    "    columns = pd.read_csv(f, nrows=1).columns\n",
    "    feature_names.extend([fea for fea in columns if fea in used_features])\n",
    "\n",
    "\n",
    "print(\"一共有%d个特征:\" % len(feature_names))\n",
    "print(feature_names)\n",
    "\n",
    "for i in range(iter_num):\n",
    "    gbm_booster = None\n",
    "    print(\" +++++++++++++++ 开始%d/%d ++++++++++++++\" % (i+1, iter_num))\n",
    "    since = time.time()\n",
    "    \n",
    "    # 提前分配所有特征的内存\n",
    "    train_fea = np.empty((chunksize, len(feature_names)), dtype=np.float32)\n",
    "    start_dim, end_dim = 0, 0\n",
    "\n",
    "    print(\"提取word2vec直接做差特征...\")\n",
    "    w2v_model_path = '/home/kesci/work/word2vec/w2v_100_cbow_4e.model'\n",
    "    w2v_model = Word2Vec.load(w2v_model_path)\n",
    "    embed_size = w2v_model.trainables.layer1_size\n",
    "    print(\"word2vec加载完毕. 总词典数: %d, embeding size: %d\" % (len(w2v_model.wv.vocab), embed_size))\n",
    "    end_dim = embed_size\n",
    "    print(\"train:\")\n",
    "    for idx, (query_words, title_words) in tqdm(enumerate(query_title_generator(train_file_path, \n",
    "                                        skiprows=chunksize * i, have_header=False, \n",
    "                                        max_rows=chunksize))):\n",
    "        train_fea[idx, start_dim:end_dim] = w2v_sent2vec(query_words, w2v_model)-w2v_sent2vec(title_words, w2v_model)\n",
    "    \n",
    "    for file in files:\n",
    "        # train feature\n",
    "        print(\"train:\", end=\"\")\n",
    "        fea = load_feature_csv(file, start_row = i * chunksize, \n",
    "                                nrows = chunksize).values.astype(np.float32)\n",
    "        start_dim = end_dim\n",
    "        end_dim = end_dim + fea.shape[1]\n",
    "        train_fea[:, start_dim:end_dim] = fea\n",
    "        del fea\n",
    "        \n",
    "    gc.collect()\n",
    "    \n",
    "    assert end_dim == len(feature_names)\n",
    "    \n",
    "    print(\"数值特征加载完毕, train shape:\", train_fea.shape)\n",
    "    label = load_feature_csv(train_label_file, start_row = i * chunksize, \n",
    "                                nrows = chunksize, \n",
    "                                drop=False)[\"label\"].values.astype(np.int32)\n",
    "    \n",
    "    print(\"spliting dataset...\", flush=True)\n",
    "    valid_num = int(chunksize * 0.2)\n",
    "    train_X, train_y = train_fea[:-valid_num, :], label[:-valid_num]\n",
    "    valid_X, valid_y = train_fea[-valid_num:, :], label[-valid_num:]\n",
    "    del train_fea, label\n",
    "    \n",
    "    gc.collect()\n",
    "    print(\"构造lgb Dataset...\", flush=True)\n",
    "    lgb_train_set = lgb.Dataset(train_X, train_y) # , categorical_feature=[\"title_labelencoder\"])\n",
    "    lgb_valid_set = lgb.Dataset(valid_X, valid_y, # categorical_feature=[\"title_labelencoder\"],\n",
    "                                reference=lgb_train_set)\n",
    "    gc.collect()\n",
    "\n",
    "    gbm_booster = lgb.train(lgb_params,\n",
    "                            lgb_train_set,\n",
    "                            num_boost_round=4000,\n",
    "                            valid_sets=[lgb_train_set, lgb_valid_set],\n",
    "                            valid_names=[\"tn\", \"vd\"],\n",
    "                            init_model=None, # 若不为None就在上次的基础上接着训练\n",
    "                            # feature_name=x_cols,\n",
    "                            early_stopping_rounds=40,\n",
    "                            # feval=feval_auc,\n",
    "                            verbose_eval=20,\n",
    "                            # learning_rates=lambda iter: max(0.01, lgb_params[\"learning_rate\"]*(0.9988**iter)),  # 学习率衰减\n",
    "                            ) # 不使用增量训练\n",
    "    time_elapsed = time.time() - since\n",
    "    print('lgb训练结束. complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    del lgb_train_set, lgb_valid_set, train_X, train_y, valid_X, valid_y\n",
    "    \n",
    "    if not debug:\n",
    "        save_name_prefix = \"/home/kesci/work/lgb_model_result/805_tss_lgb_noCVR_%d\" % i\n",
    "        gbm_booster.save_model(save_name_prefix + '_booster.txt')\n",
    "        print(\"Model save done!\")\n",
    "        \n",
    "        fea_impt = gbm_booster.feature_importance()\n",
    "        fea_impt_df = pd.DataFrame(data={\"feature\": feature_names, \"importance\": fea_impt})\n",
    "        fea_impt_df.to_csv(save_name_prefix + '_fea_impt.csv', index=False)\n",
    "        print(\"feature importances df save done.\")\n",
    "    \n",
    "    del gbm_booster\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2E3E8F68DCF47BEA2C867649010353E"
   },
   "source": [
    "## 3.2 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "1BB2B20F41384B8583DED81735696F48"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics.pairwise import paired_distances\n",
    "from sklearn import metrics\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models import Word2Vec\n",
    "# from glove import Glove\n",
    "import pickle\n",
    "# from sklearn.externals import joblib\n",
    "import os\n",
    "import csv\n",
    "import psutil\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "import math\n",
    "\n",
    "if not os.path.isdir('lgb_model_result'):\n",
    "    os.mkdir('lgb_model_result')\n",
    "\n",
    "TEST_A_ROWS, TEST_B_ROWS = 20000000, 100000000\n",
    "\n",
    "\n",
    "chunksize=None\n",
    "debug = False\n",
    "if debug:\n",
    "    chunksize = 100000\n",
    "    iter_num = 2\n",
    "\n",
    "\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "test_A_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "test_B_file_path = '/home/kesci/input/bytedance/bytedance_contest.final_2.csv'\n",
    "\n",
    "train_columns = ['query_id','query','query_title_id','title','label']\n",
    "test_columns = ['query_id','query','query_title_id','title']\n",
    "print(\"当前进程PID:\", os.getpid(), \"开始时间:\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "\n",
    "\n",
    "# 这个文件存放了训练集label\n",
    "train_label_file = \"/home/kesci/work/feature/train_label.csv.gz\"\n",
    "save_dir = \"/home/kesci/work/feature\"\n",
    "\n",
    "# files = [os.path.join(save_dir, 'base_feature_v2_all_%d.csv'%i) for i in range(5)]\n",
    "files = [\n",
    "    \"/home/kesci/work/feature/feature_1_0.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_2.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_3.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_4.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_5.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_6.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_7.csv.gz\",\n",
    "    \n",
    "    \"/home/kesci/work/feature_4e/feature_1_1_4e.csv.gz\",\n",
    "    \"/home/kesci/work/feature_4e/feature_9_0_4e.csv.gz\",\n",
    "    \"/home/kesci/work/feature_4e/feature_9_1_4e.csv.gz\",\n",
    "    \"/home/kesci/work/feature_4e/feature_9_0_300d_4e.csv.gz\",\n",
    "    \"/home/kesci/work/feature_4e/feature_9_1_300d_4e.csv.gz\",\n",
    "    # cvr\n",
    "    \"/home/kesci/work/feature_4e/feature_8_4e.csv.gz\",\n",
    " \n",
    "    \"/home/kesci/work/feature_4e/feature_10_0th_2e.csv.gz\", # 0-2亿\n",
    "    # \"/home/kesci/work/feature_4e/feature_10_1th_2e.csv.gz\", # 2-4亿\n",
    "]\n",
    "\n",
    "used_features = [\n",
    "# feature 1_0\n",
    "'query_length','title_length','WordMatchShare','WordMatchShare_query',\n",
    "'WordMatchShare_title',\n",
    "'LengthDiff', 'LengthDiffRate', 'LengthRatio_qt', 'LengthRatio_tq', # 这四个根据前面的计算得到\n",
    "\n",
    "# feature 1_1\n",
    "'TFIDFWordMatchShare','TFIDFWordMatchShare_query','TFIDFWordMatchShare_title',\n",
    "\n",
    "\n",
    "# feature 2\n",
    "'NgramJaccardCoef_1', 'NgramJaccardCoef_2', 'NgramJaccardCoef_3', 'NgramJaccardCoef_4',\n",
    "\n",
    "# feature 3\n",
    "'Levenshtein_ratio', 'Levenshtein_distance_char','query_title_common_words', 'common_word_ratio',\n",
    "\n",
    "# feature 4\n",
    "'lcsubstr_len', 'lcseque_len','longest_match_size', 'longest_match_ratio',\n",
    "\n",
    "# feature 5\n",
    "'fuzz_qratio', 'fuzz_partial_ratio',\n",
    "\n",
    "#feature 6\n",
    "'fuzz_partial_token_sort_ratio','fuzz_token_set_ratio','fuzz_token_sort_ratio',\n",
    "\n",
    "#feature 7\n",
    "'query_Entropy','title_Entropy','query_title_Entropy','WordMatchShare_Entropy',\n",
    "\n",
    "#feature 8\n",
    "\"query_convert\", \"title_convert\", \"query_title_convert\",\n",
    "\n",
    "# fature 9_0\n",
    "'w2v_avg_cosine', 'w2v_avg_cityblock',\n",
    "\n",
    "# feature 9_1\n",
    "'w2v_avg_minkowski', 'w2v_avg_braycurtis', 'w2v_avg_canberra',\n",
    "\n",
    "# fature 9_0 300d\n",
    "'w2v300_avg_cosine', 'w2v300_avg_cityblock',\n",
    "\n",
    "# feature 9_1 300d\n",
    "'w2v300_avg_minkowski', 'w2v300_avg_braycurtis', 'w2v300_avg_canberra',\n",
    "\n",
    "# feature 10\n",
    "'query_title_click', 'query_nunique_title', 'query_click', 'title_nunique_query', 'title_click',\n",
    "]\n",
    "\n",
    "# gensim feature\n",
    "class EpochSaver(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    '''用于保存模型, 打印损失函数等等'''\n",
    "    def __init__(self, savedir, save_name=\"word2vector.model\"):\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        self.save_path = os.path.join(savedir, save_name)\n",
    "        self.epoch = 0\n",
    "        self.pre_loss = 0\n",
    "        self.best_loss = 999999999.9\n",
    "        self.since = time.time()\n",
    "        \n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        cum_loss = model.get_latest_training_loss() # 返回的是从第一个epoch累计的\n",
    "        epoch_loss = cum_loss - self.pre_loss\n",
    "        time_taken = time.time() - self.since\n",
    "        print(\"Epoch %d, loss: %.2f, time: %dmin %ds\" % \n",
    "                    (self.epoch, epoch_loss, time_taken//60, time_taken%60))\n",
    "        if self.best_loss > epoch_loss:\n",
    "            self.best_loss = epoch_loss\n",
    "            print(\"Better model. Best loss: %.2f\" % self.best_loss)\n",
    "            model.save(self.save_path)\n",
    "            print(\"Model %s save done!\" % self.save_path)\n",
    "            \n",
    "        self.pre_loss = cum_loss\n",
    "        self.since = time.time()\n",
    "        print(\"\\n\\n\\n\")\n",
    "\n",
    "def query_title_generator(csv_file_path, skiprows=0, have_header=False, max_rows=None):\n",
    "    if max_rows is None:\n",
    "        max_rows = -1\n",
    "    with open(csv_file_path) as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        if have_header:\n",
    "            print(\"跳过表头!!!\")\n",
    "            next(csv_reader) # 跳过表头\n",
    "        for _ in range(skiprows):\n",
    "            next(csv_reader)\n",
    "        \n",
    "        count = 0\n",
    "        for line in csv_reader:\n",
    "            if count == max_rows:\n",
    "                return\n",
    "            count += 1\n",
    "            yield line[1].strip().split(), line[3].strip().split() # query, title\n",
    "\n",
    "def w2v_sent2vec(words, model):\n",
    "    \"\"\"计算句子的平均word2vec向量, sentences是一个句子, 句向量最后会归一化\"\"\"\n",
    "    M = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            M.append(model.wv[word])\n",
    "        except KeyError: # 不在词典里\n",
    "            continue\n",
    "    \n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return (v / np.sqrt((v ** 2).sum())).astype(np.float32)\n",
    "\n",
    "def load_feature_csv(csv_file, start_row=0, nrows=chunksize, drop=True):\n",
    "    \"\"\"如果是读取特征文件那么drop=True以丢弃不用的feature, 否则应该设成False, 例如读id文件\"\"\"\n",
    "    since = time.time()\n",
    "    base_feature_df = pd.read_csv(csv_file, dtype=\"float32\", names=None, \n",
    "                                skiprows=start_row, nrows=nrows)\n",
    "    df_len = base_feature_df.shape[0]\n",
    "    print('load %d rows from %s(start from row %d)' \\\n",
    "                        % (df_len, csv_file.split(\"/\")[-1], start_row), end=\" \")\n",
    "                        \n",
    "    columns = pd.read_csv(csv_file, dtype=\"float32\", nrows=1).columns\n",
    "    base_feature_df.columns = columns\n",
    "    print(columns.values)    \n",
    "    unused_features = [fea for fea in columns if fea not in used_features]\n",
    "    if drop and len(unused_features) != 0:\n",
    "        print(\"\\n丢弃掉feature:\", unused_features, end=\"\")\n",
    "        base_feature_df = base_feature_df.drop(unused_features, axis=1)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n",
    "    return base_feature_df\n",
    "    \n",
    "\n",
    "feature_num = 151\n",
    "print(\"一共有%d个特征(一定要和训练保持一致!!!)\" % feature_num)\n",
    "\n",
    "\n",
    "print(\" +++++++++++++++ 开始提取特征(一定要和训练保持完全一致) ++++++++++++++\")\n",
    "since = time.time()\n",
    "\n",
    "print(\"---- 提取word2vec直接做差特征...\")\n",
    "w2v_model_path = '/home/kesci/work/word2vec/w2v_100_cbow_4e.model'\n",
    "w2v_model = Word2Vec.load(w2v_model_path)\n",
    "embed_size = w2v_model.trainables.layer1_size\n",
    "print(\"word2vec加载完毕. 总词典数: %d, embeding size: %d\" % (len(w2v_model.wv.vocab), embed_size))\n",
    "\n",
    "print(\"---- 提取其他特征:\")\n",
    "if debug:\n",
    "    TEST_A_ROWS, TEST_B_ROWS = 100000, 100000\n",
    "test_A_fea = np.zeros((TEST_A_ROWS, feature_num), dtype=np.float32)\n",
    "test_B_fea = np.zeros((TEST_B_ROWS, feature_num), dtype=np.float32)\n",
    "start_dim, end_dim = 0, 0\n",
    "\n",
    "print(\"word2vec距离\")\n",
    "print(\"-- test A:\")\n",
    "end_dim = embed_size\n",
    "for idx, (query_words, title_words) in tqdm(enumerate(query_title_generator(test_A_file_path, \n",
    "                                    skiprows=0, have_header=False, \n",
    "                                    max_rows=TEST_A_ROWS))):\n",
    "    test_A_fea[idx, start_dim:end_dim] = w2v_sent2vec(query_words, w2v_model) \\\n",
    "                        - w2v_sent2vec(title_words, w2v_model)\n",
    "print('complete with idx %d.' % idx)\n",
    "print(\"-- test B:\")                      \n",
    "for idx, (query_words, title_words) in tqdm(enumerate(query_title_generator(test_B_file_path, \n",
    "                                    skiprows=0, have_header=False, \n",
    "                                    max_rows=TEST_B_ROWS))):\n",
    "    test_B_fea[idx, start_dim:end_dim] = w2v_sent2vec(query_words, w2v_model) \\\n",
    "                        - w2v_sent2vec(title_words, w2v_model)\n",
    "print('complete with idx %d.' % idx)\n",
    "                        \n",
    "del w2v_model\n",
    "gc.collect()\n",
    "                        \n",
    "print(\"其他特征(包括前两亿的特征10):\")\n",
    "feature10_first2e_tag = True\n",
    "for file in files:\n",
    "    fea_A = load_feature_csv(file.replace(\".csv\", \"_testA.csv\"), start_row = 0, \n",
    "                            nrows = TEST_A_ROWS).values.astype(np.float32)\n",
    "    start_dim = end_dim\n",
    "    end_dim = end_dim + fea_A.shape[1]\n",
    "    test_A_fea[:, start_dim:end_dim] = fea_A\n",
    "    \n",
    "\n",
    "    fea_B = load_feature_csv(file.replace(\".csv\", \"_testB.csv\"), start_row = 0, \n",
    "                            nrows = TEST_B_ROWS).values.astype(np.float32)\n",
    "    test_B_fea[:, start_dim:end_dim] = fea_B  \n",
    "    del fea_A, fea_B\n",
    "    \n",
    "assert end_dim == feature_num\n",
    "\n",
    "\n",
    "for chunk_idx in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "    save_name_prefix = \"/home/kesci/work/lgb_model_result/805_tss_lgb_\" + str(chunk_idx)\n",
    "    print(\"chunk_idx =\", chunk_idx)\n",
    "    if chunk_idx < 5:\n",
    "        assert feature10_first2e_tag\n",
    "        print(\"注意feature10使用的是前2亿的!!!!\")\n",
    "    elif chunk_idx >= 5:\n",
    "        if feature10_first2e_tag:\n",
    "            print(\"提取后两亿对应的测试集的feature10...\")\n",
    "            fea10_file = \"/home/kesci/work/feature_4e/feature_10_1th_2e.csv.gz\"\n",
    "            fea_A = load_feature_csv(fea10_file.replace(\".csv\", \"_testA.csv\"), start_row = 0, \n",
    "                            nrows = TEST_A_ROWS).values.astype(np.float32)\n",
    "            test_A_fea[:, feature_num - 5:] = fea_A # feature10一共有5个特征\n",
    "            \n",
    "            fea_B = load_feature_csv(fea10_file.replace(\".csv\", \"_testB.csv\"), start_row = 0, \n",
    "                                    nrows = TEST_B_ROWS).values.astype(np.float32)\n",
    "            test_B_fea[:, feature_num - 5:] = fea_B    \n",
    "            del fea_A, fea_B\n",
    "            feature10_first2e_tag = False\n",
    "        print(\"注意feature10使用的是后2亿的!!!!\")\n",
    "\n",
    "    model_file = save_name_prefix + '_booster.txt'\n",
    "    print(\"从%s加载lgb模型数据...\" % model_file)\n",
    "    gbm_booster = lgb.Booster(model_file = model_file)\n",
    "    \n",
    "    test_A_pred = gbm_booster.predict(test_A_fea)\n",
    "    test_B_pred = gbm_booster.predict(test_B_fea)\n",
    "    result_A_df = pd.DataFrame({\"prediction\": test_A_pred}, dtype=np.float32)\n",
    "    result_B_df = pd.DataFrame({\"prediction\": test_B_pred}, dtype=np.float32)\n",
    "    print(\"test A:\")\n",
    "    print(result_A_df.shape)\n",
    "    print(result_A_df.head(10))\n",
    "    print(\"testB:\")\n",
    "    print(result_B_df.shape)\n",
    "    print(result_B_df.head(50))\n",
    "    del test_A_pred\n",
    "    del test_B_pred, gbm_booster\n",
    "        \n",
    "    if not debug:\n",
    "        result_A_df.to_csv(save_name_prefix + \"_testA.csv.gz\", compression='gzip', index=False, header=False)\n",
    "        print(\"testA Save done!\")\n",
    "        result_B_df.to_csv(save_name_prefix + \"_testB.csv.gz\", compression='gzip', index=False, header=False)\n",
    "        print(\"TestB Save Done!\")\n",
    "    \n",
    "    del result_A_df\n",
    "    del result_B_df\n",
    "    gc.collect()\n",
    "del test_A_fea\n",
    "del test_B_fea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89664A643926480384624DECD43F3B5B"
   },
   "source": [
    "# 4 ESIM加外部特征(pytorch)\n",
    "* 外部特征使用了除转换率外所有特征\n",
    "* 下面代码使用的是100d的词向量，实际还用了300d的\n",
    "* 和lgb一样，由于特征10是分成了两个2亿，所以前两亿和后两亿的代码略有不同，下面的代码是前2亿的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "1748CAC4F1B34A039A44120C798F76DB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import gensim\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "train_columns = ['query_id', 'query', 'query_title_id', 'title', 'label']\n",
    "test_columns = ['query_id', 'query', 'query_title_id', 'title']\n",
    "\n",
    "TEST_A_ROWS, TEST_B_ROWS = 20000000, 100000000\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "test_A_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "test_B_file_path = '/home/kesci/input/bytedance/bytedance_contest.final_2.csv'\n",
    "\n",
    "print(\"当前进程PID:\", os.getpid(), \n",
    "        \"开始时间:\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "\n",
    "\n",
    "args = {\n",
    "    \"w2v_model_path\": \"/home/kesci/work/word2vec/w2v_100_cbow_4e.model\",\n",
    "    \"model_save_dir\": \"/home/kesci/work/tss_nn/tss_esim100_811\",\n",
    "    \n",
    "    \"dropout\": 0.5,\n",
    "    \"hidden_size\": 128,\n",
    "    \"linear_size\": 256\n",
    "}\n",
    "\n",
    "# model_save_path = \"/home/kesci/work/nn_related/tss_rnn_727.pth\"\n",
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "MAX_QUERY_LEN = 10\n",
    "MAX_TITLE_LEN = 20\n",
    "PAD, OOV = 0, 1\n",
    "\n",
    "E2 = 200000000  # 2亿\n",
    "chunksize = E2\n",
    "# assert E2 % chunksize == 0\n",
    "\n",
    "debug = False\n",
    "if debug:\n",
    "    chunksize = 1000000\n",
    "    chunks = 2\n",
    "\n",
    "feature_files = [\n",
    "    \"/home/kesci/work/feature/feature_1_0.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_2.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_3.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_4.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_5.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_6.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_7.csv.gz\",\n",
    "    \n",
    "    \"/home/kesci/work/feature_4e/feature_1_1_4e.csv.gz\",\n",
    "    \"/home/kesci/work/feature_4e/feature_9_0_4e.csv.gz\",\n",
    "    \"/home/kesci/work/feature_4e/feature_9_1_4e.csv.gz\",\n",
    "    \"/home/kesci/work/feature_4e/feature_9_0_300d_4e.csv.gz\",\n",
    "    \"/home/kesci/work/feature_4e/feature_9_1_300d_4e.csv.gz\",\n",
    "    \n",
    "    # \"/home/kesci/work/feature_4e/feature_10_1th_2e.csv.gz\", # 0-2亿\n",
    "    \"/home/kesci/work/feature_4e/feature_10_0th_2e.csv.gz\", # 0-2亿\n",
    "\n",
    "]\n",
    "print(\"注意feature10使用的是前2亿的!!!!\")\n",
    "\n",
    "fea_names = []\n",
    "for file in feature_files:\n",
    "    fea_names.extend(pd.read_csv(file, nrows=2).columns)\n",
    "print(\"一共%d个外部特征:\" % len(fea_names), fea_names)\n",
    "\n",
    "\n",
    "class ESIM(nn.Module):\n",
    "    def __init__(self, manual_feature_dim, vocab_size, embed_dim, hidden_size, linear_size, dropout=0.5):\n",
    "        super(ESIM, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embeds = nn.Embedding(vocab_size, self.embed_dim)\n",
    "        # self.bn_embeds = nn.BatchNorm1d(self.embed_dim, momentum=0.01)\n",
    "        self.lstm1 = nn.LSTM(self.embed_dim, self.hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(self.hidden_size*8, self.hidden_size, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            # nn.BatchNorm1d(self.hidden_size * 8 + manual_feature_dim, momentum=0.01),\n",
    "            nn.Linear(self.hidden_size * 8 + manual_feature_dim, linear_size),\n",
    "            nn.ELU(inplace=True),\n",
    "            # nn.BatchNorm1d(linear_size, momentum=0.01),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(linear_size, linear_size),\n",
    "            nn.ELU(inplace=True),\n",
    "            # nn.BatchNorm1d(linear_size, momentum=0.01),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(linear_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def soft_attention_align(self, x1, x2, mask1, mask2):\n",
    "        '''\n",
    "        x1: batch_size * seq_len * dim\n",
    "        x2: batch_size * seq_len * dim\n",
    "        '''\n",
    "        # attention: batch_size * seq_len * seq_len\n",
    "        attention = torch.matmul(x1, x2.transpose(1, 2))\n",
    "        mask1 = mask1.float().masked_fill_(mask1, float('-inf'))\n",
    "        mask2 = mask2.float().masked_fill_(mask2, float('-inf'))\n",
    "\n",
    "        # weight: batch_size * seq_len * seq_len\n",
    "        weight1 = F.softmax(attention + mask2.unsqueeze(1), dim=-1)\n",
    "        x1_align = torch.matmul(weight1, x2)\n",
    "        weight2 = F.softmax(attention.transpose(1, 2) + mask1.unsqueeze(1), dim=-1)\n",
    "        x2_align = torch.matmul(weight2, x1)\n",
    "        # x_align: batch_size * seq_len * hidden_size\n",
    "\n",
    "        return x1_align, x2_align\n",
    "\n",
    "    def submul(self, x1, x2):\n",
    "        mul = x1 * x2\n",
    "        sub = x1 - x2\n",
    "        return torch.cat([sub, mul], -1)\n",
    "\n",
    "    def apply_multiple(self, x):\n",
    "        # input: batch_size * seq_len * (2 * hidden_size)\n",
    "        p1 = F.avg_pool1d(x.transpose(1, 2), x.size(1)).squeeze(-1)\n",
    "        p2 = F.max_pool1d(x.transpose(1, 2), x.size(1)).squeeze(-1)\n",
    "        # output: batch_size * (4 * hidden_size)\n",
    "        return torch.cat([p1, p2], 1)\n",
    "\n",
    "    def forward(self, sent1, sent2, manual_feature):\n",
    "        # batch_size * seq_len\n",
    "        mask1, mask2 = sent1.eq(PAD), sent2.eq(PAD)\n",
    "\n",
    "        # embeds: batch_size * seq_len => batch_size * seq_len * dim\n",
    "        x1 = self.embeds(sent1)\n",
    "        x2 = self.embeds(sent2)\n",
    "\n",
    "        # batch_size * seq_len * dim => batch_size * seq_len * hidden_size\n",
    "        o1, _ = self.lstm1(x1)\n",
    "        o2, _ = self.lstm1(x2)\n",
    "\n",
    "        # Attention\n",
    "        # batch_size * seq_len * hidden_size\n",
    "        q1_align, q2_align = self.soft_attention_align(o1, o2, mask1, mask2)\n",
    "        \n",
    "        # Compose\n",
    "        # batch_size * seq_len * (8 * hidden_size)\n",
    "        q1_combined = torch.cat([o1, q1_align, self.submul(o1, q1_align)], -1)\n",
    "        q2_combined = torch.cat([o2, q2_align, self.submul(o2, q2_align)], -1)\n",
    "\n",
    "        # batch_size * seq_len * (2 * hidden_size)\n",
    "        q1_compose, _ = self.lstm2(q1_combined)\n",
    "        q2_compose, _ = self.lstm2(q2_combined)\n",
    "\n",
    "        # Aggregate\n",
    "        # input: batch_size * seq_len * (2 * hidden_size)\n",
    "        # output: batch_size * (4 * hidden_size)\n",
    "        q1_rep = self.apply_multiple(q1_compose)\n",
    "        q2_rep = self.apply_multiple(q2_compose)\n",
    "\n",
    "        # Classifier\n",
    "        x = torch.cat([q1_rep, q2_rep, manual_feature], -1)\n",
    "        similarity = self.fc(x)\n",
    "        return similarity\n",
    "        \n",
    "        \n",
    "# ######################################### 2. 加载数据 ##########################################\n",
    "def load_feature_csv(csv_file, start_row, nrows):\n",
    "    base_feature_df = pd.read_csv(csv_file, dtype=\"float32\", names=None, \n",
    "                                skiprows=start_row, nrows=nrows)\n",
    "    df_len = base_feature_df.shape[0]\n",
    "    print('load %d rows from %s(start from row %d)' \\\n",
    "                        % (df_len, csv_file.split(\"/\")[-1], start_row), flush=True)\n",
    "\n",
    "    return base_feature_df\n",
    "    \n",
    "class myDataloader(object):\n",
    "    def __init__(self, file_path, fea_file_path_list, stoi, bigbatch,\n",
    "                    skiprows = 0, nrows=None,  batch_size=128, shuffle=True, columns=train_columns):\n",
    "        self.file_path = file_path\n",
    "        self.fea_file_path_list = fea_file_path_list\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.stoi = stoi\n",
    "        self.bigbatch = bigbatch\n",
    "        self.skiprows = skiprows\n",
    "        self.nrows = nrows\n",
    "        self.columns = columns\n",
    "        self.feature_num = self._get_feature_num()\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "        先按照顺序读取一个大bigbatch进内存, 再随机从大bigbatch里取一个batch(最后一个batch可能不够batch_size)\n",
    "        \"\"\"\n",
    "        print(\"从%s加载原始数据...\" % self.file_path)\n",
    "        df_iter = pd.read_csv(self.file_path, nrows=self.nrows, header=None, \n",
    "                                skiprows=self.skiprows, chunksize=self.bigbatch)\n",
    "        count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                df = df_iter.get_chunk()\n",
    "            except StopIteration:\n",
    "                return\n",
    "            df.columns = self.columns\n",
    "\n",
    "            if \"label\" in self.columns:\n",
    "                label = df['label'].values.astype(np.int32)\n",
    "            else:\n",
    "                label = -1 * np.zeros(df.shape[0], dtype=np.int32)\n",
    "            query_lens = []\n",
    "            title_lens = []\n",
    "            query_array = np.empty((df.shape[0], MAX_QUERY_LEN), dtype=np.int32)\n",
    "            title_array = np.empty((df.shape[0], MAX_TITLE_LEN), dtype=np.int32)\n",
    "\n",
    "            for i, query in enumerate(df[\"query\"].apply(lambda x: x.strip().split())):\n",
    "                query = [self.stoi.get(w, OOV) for w in query]\n",
    "                if len(query) > MAX_QUERY_LEN:\n",
    "                    query = query[:MAX_QUERY_LEN]\n",
    "                q_len = len(query)\n",
    "                query_lens.append(q_len)\n",
    "                if q_len < MAX_QUERY_LEN:\n",
    "                    query = query + [PAD]*(MAX_QUERY_LEN - q_len)\n",
    "                query_array[i, :] = np.array(query)\n",
    "\n",
    "            for i, title in enumerate(df[\"title\"].apply(lambda x: x.strip().split())):\n",
    "                title = [self.stoi.get(w, OOV) for w in title]\n",
    "                if len(title) > MAX_TITLE_LEN:\n",
    "                    title = title[:MAX_TITLE_LEN]\n",
    "                t_len = len(title)\n",
    "                title_lens.append(t_len)\n",
    "                if t_len < MAX_TITLE_LEN:\n",
    "                    title = title + [PAD]*(MAX_TITLE_LEN - t_len)\n",
    "                title_array[i, :] = np.array(title)\n",
    "\n",
    "            del df\n",
    "            gc.collect()\n",
    "            query_lens = np.array(query_lens)\n",
    "            title_lens = np.array(title_lens)\n",
    "\n",
    "            fea_array = np.empty((query_array.shape[0], self.feature_num), dtype=np.float32)\n",
    "            start_dim, end_dim = 0, 0\n",
    "            for file in self.fea_file_path_list:\n",
    "                start_row = self.skiprows + count * self.bigbatch\n",
    "                if \"feature_10_1th_2e.csv.gz\" in file:\n",
    "                    start_row -= E2\n",
    "                fea = load_feature_csv(file, start_row = start_row, \n",
    "                                        nrows = query_array.shape[0]).values.astype(np.float32)\n",
    "                start_dim = end_dim\n",
    "                end_dim = end_dim + fea.shape[1]\n",
    "                fea_array[:, start_dim:end_dim] = fea\n",
    "                del fea\n",
    "            assert end_dim == self.feature_num\n",
    "            count += 1\n",
    "\n",
    "            idxes = list(range(query_array.shape[0]))\n",
    "            if self.shuffle:\n",
    "                random.shuffle(idxes)\n",
    "            for b in range(math.ceil(query_array.shape[0] / self.batch_size)):\n",
    "                start = b * self.batch_size\n",
    "                end = min((b+1)*self.batch_size, query_array.shape[0])\n",
    "\n",
    "                yield torch.tensor(query_lens[idxes[start:end]], dtype=torch.int),\\\n",
    "                        torch.tensor(title_lens[idxes[start:end]], dtype=torch.int),\\\n",
    "                        torch.tensor(query_array[idxes[start:end]], dtype=torch.long),\\\n",
    "                        torch.tensor(title_array[idxes[start:end]], dtype=torch.long),\\\n",
    "                        torch.tensor(fea_array[idxes[start:end]], dtype=torch.float32),\\\n",
    "                        torch.tensor(label[idxes[start:end]], dtype=torch.float)\n",
    "            \n",
    "    def _get_feature_num(self):\n",
    "        feature_num = 0\n",
    "        for file in self.fea_file_path_list:\n",
    "            feature_num += pd.read_csv(file, nrows=2).shape[1]\n",
    "        return feature_num\n",
    "\n",
    "# ################################### 3. 训练 #####################################\n",
    "def evaluate(dataloader, net, loss):\n",
    "    net.eval()\n",
    "    loss_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for query_lens, title_lens, query, title, fea, y in dataloader.next_batch():\n",
    "            query = query.to(device)\n",
    "            title = title.to(device)\n",
    "            fea = fea.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_hat = net(query, title, fea)\n",
    "            loss_sum += loss(y_hat.view(y.shape), y).cpu().item()\n",
    "            n += 1\n",
    "    net.train()\n",
    "    return loss_sum / n\n",
    "\n",
    "def train(net, loss, optimizer, scheduler, stoi, num_epochs, save_path, \n",
    "            verbose=1000, batch_size=512, bigbatch=128*10000, \n",
    "            valid_portion=0.2, valid_every=10000):\n",
    "    net.train()\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    # print(\"从之前的0.4111开始\")\n",
    "    best_valid_loss = 99999.9\n",
    "    print(\"使用%.3f的数据作为验证集, 每%d步验证一次\" % (valid_portion, valid_every))\n",
    "    valid_num = int(chunksize * valid_portion)\n",
    "    train_num = chunksize - valid_num\n",
    "    \n",
    "    # print(\"从2亿的数据开始!!!!!!\")\n",
    "    offset = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_since = time.time()\n",
    "        train_loader = myDataloader(train_file_path, feature_files, stoi, bigbatch,\n",
    "                    skiprows=offset, nrows=train_num,  batch_size=batch_size, shuffle=True, columns=train_columns)\n",
    "        valid_loader = myDataloader(train_file_path, feature_files, stoi, bigbatch,\n",
    "                    skiprows=offset + train_num, nrows=valid_num,  batch_size=batch_size, shuffle=False, columns=train_columns)\n",
    "\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        train_loss = []\n",
    "        for step, (query_lens, title_lens, query, title, fea, y) in enumerate(train_loader.next_batch()):\n",
    "            step_since = time.time()\n",
    "            query = query.to(device)\n",
    "            title = title.to(device)\n",
    "            fea = fea.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_hat = net(query, title, fea)\n",
    "            l = loss(y_hat.view(y.shape), y) \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(l.cpu().item())\n",
    "            if (step + 1) % verbose == 0:\n",
    "                train_avg_loss = sum(train_loss) / len(train_loss)\n",
    "                train_loss = []\n",
    "                print(\"epoch %d, step %d, train avg loss: %.4f, step time taken: %.2f sec\" \\\n",
    "                    % (epoch+1, step+1, train_avg_loss, time.time() - step_since))\n",
    "                    \n",
    "            # validate\n",
    "            if (step + 1) % valid_every == 0:\n",
    "                print(\"开始验证...\")\n",
    "                valid_loss = evaluate(valid_loader, net, loss)\n",
    "                scheduler.step()\n",
    "                print(\"epoch %d, step %d, valid loss: %.4f, best valid loss: %.4f\" \\\n",
    "                    % (epoch+1, step+1, valid_loss, best_valid_loss))\n",
    "                if best_valid_loss > valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    if not debug:\n",
    "                        print(\"better model! saving to %s...\" % save_path, end=\"\")\n",
    "                        torch.save(net.state_dict(), save_path)\n",
    "                        print(\"done!\\n\")\n",
    "                \n",
    "                \n",
    "def get_parameter_number(net):\n",
    "    total_num = sum(p.numel() for p in net.parameters())\n",
    "    trainable_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    return {'Total': total_num, 'Trainable': trainable_num}\n",
    "            \n",
    "            \n",
    "class EpochSaver(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    '''用于保存w2v模型, 打印损失函数等等'''\n",
    "    def __init__(self, savedir, save_name=\"word2vector.model\"):\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        self.save_path = os.path.join(savedir, save_name)\n",
    "        self.epoch = 0\n",
    "        self.pre_loss = 0\n",
    "        self.best_loss = 999999999.9\n",
    "        self.since = time.time()\n",
    "        \n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        cum_loss = model.get_latest_training_loss() # 返回的是从第一个epoch累计的\n",
    "        epoch_loss = cum_loss - self.pre_loss\n",
    "        time_taken = time.time() - self.since\n",
    "        print(\"Epoch %d, loss: %.2f, time: %dmin %ds\" % \n",
    "                    (self.epoch, epoch_loss, time_taken//60, time_taken%60))\n",
    "        if self.best_loss >= epoch_loss:\n",
    "            self.best_loss = epoch_loss\n",
    "            print(\"Better model. Best loss: %.2f\" % self.best_loss)\n",
    "            model.save(self.save_path)\n",
    "            print(\"Model %s save done!\" % self.save_path)\n",
    "\n",
    "        self.pre_loss = cum_loss\n",
    "        self.since = time.time()\n",
    "        print(\"\\n\\n\\n\")\n",
    "        \n",
    "print(\"加载word2vec模型\")\n",
    "w2v_model = Word2Vec.load(args[\"w2v_model_path\"])\n",
    "embed_size = w2v_model.trainables.layer1_size\n",
    "vocab_size = len(w2v_model.wv.vocab)\n",
    "print(\"加载完毕. 总词典数(不算PAD和OOV): %d, embeding size: %d\" % (vocab_size, embed_size))\n",
    "words = list(w2v_model.wv.vocab.keys())\n",
    "word2id = {word:(i+2) for i, word in enumerate(words)} # 2, 3, 4..., 01留给PAD和OOV\n",
    "\n",
    "\n",
    "print(\"构造embedding层\")\n",
    "embedding_matrix = np.zeros((vocab_size + 2, embed_size), dtype=np.float32)\n",
    "for word, i in tqdm(word2id.items()):\n",
    "    # try:\n",
    "    embedding_matrix[i] = w2v_model.wv[word]\n",
    "    # except:\n",
    "    #     continue\n",
    "print(embedding_matrix.shape)\n",
    "del w2v_model\n",
    "\n",
    "\n",
    "model = ESIM(manual_feature_dim = len(fea_names),\n",
    "            vocab_size = vocab_size + 2,\n",
    "            embed_dim = embed_size,\n",
    "            hidden_size = args[\"hidden_size\"],\n",
    "            linear_size = args[\"linear_size\"],\n",
    "            dropout = args[\"dropout\"])\n",
    "\n",
    "\n",
    "model.embeds.weight.data.copy_(torch.tensor(embedding_matrix))\n",
    "model.embeds.weight.requires_grad = False # 不更新embedding\n",
    "del embedding_matrix\n",
    "gc.collect()\n",
    "\n",
    "print(\"统计参数个数:\\n\", get_parameter_number(model))\n",
    "\n",
    "loss = torch.nn.BCELoss()\n",
    "lr, num_epochs = 0.001, 50 \n",
    "# 要过滤掉不计算梯度的embedding参数\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=1,gamma=0.8) #每过1step，学习率就乘gamma\n",
    "\n",
    "verbose, batch_size, bigbatch = 1000, 4096, 4096*10000\n",
    "if debug:\n",
    "    print(\"debug mode!\")\n",
    "    verbose, batch_size, bigbatch = 100, 4096, 4096*100\n",
    "    args[\"model_save_dir\"] = \"/home/kesci/work/tss_nn/esim100_debug\"\n",
    "    \n",
    "    \n",
    "    \n",
    "os.makedirs(args[\"model_save_dir\"], exist_ok=True)\n",
    "model_save_path = os.path.join(args[\"model_save_dir\"], \"model.pth\")\n",
    "train(model, loss, optimizer, scheduler, word2id, num_epochs, model_save_path, \n",
    "                        verbose=verbose, batch_size=batch_size, bigbatch=bigbatch,\n",
    "                        valid_portion=0.1, valid_every=(100 if debug else 10000))\n",
    "\n",
    "\n",
    "# 注释掉上面的train函数调用，反注释下面的代码即可进行测试\n",
    "\n",
    "# ----------------------- 下面是测试代码（1亿测试集的，2千万测试集的类似） ----------------------\n",
    "# pretrained_path = \"/home/kesci/work/tss_nn/tss_esim100_811/model.pth\"\n",
    "# print(\"从%s加载模型...\" % pretrained_path)\n",
    "# model.load_state_dict(torch.load(pretrained_path, map_location=device))\n",
    "\n",
    "# test_loader = myDataloader(test_B_file_path, [f.replace(\".csv\", \"_testB.csv\") for f in feature_files], \n",
    "#                             word2id, bigbatch=20000000, skiprows=0, nrows=None, \n",
    "#                             batch_size=4096, \n",
    "#                             shuffle=False, columns=test_columns)\n",
    "# model = model.to(device)\n",
    "# print(\"testing on \", device)\n",
    "# predicts = np.zeros((TEST_B_ROWS, ), dtype=np.float32)\n",
    "# start, end = 0, 0\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for query_lens, title_lens, query, title, fea, y in tqdm(test_loader.next_batch()):\n",
    "#         query = query.to(device)\n",
    "#         title = title.to(device)\n",
    "#         fea = fea.to(device)\n",
    "#         pred = model(query, title, fea).cpu().view(-1).detach().numpy()\n",
    "        \n",
    "#         end = start + pred.shape[0]\n",
    "#         predicts[start:end] = pred\n",
    "#         start = end\n",
    "\n",
    "# assert end == predicts.shape[0]\n",
    "\n",
    "# df = pd.DataFrame(data={\"prediction\": predicts})\n",
    "# print(df.info())\n",
    "# print(df.head(50))\n",
    "# print(df.tail(50))\n",
    "\n",
    "# df.to_csv(\"/home/kesci/work/nn_related/tss_esim100_811_4146_testB.csv.gz\", \n",
    "#             compression='gzip', index=False, header=False)\n",
    "# print(\"save done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A2D3B0FD187424BAB824A61F514BA90"
   },
   "source": [
    "# 4.1 nn模型(2层lstm然后特征交互)+外部特征(keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "68525E449D4F481686CF93BF6E1ECEC6"
   },
   "outputs": [],
   "source": [
    "##  hc nn model-复赛\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm as tqdm \n",
    "import time, csv, json, os, math, gc, random\n",
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import * \n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "w2v_model_path = '/home/kesci/work/word2vec/w2v_100_cbow_4e.model'\n",
    "\n",
    "print(\"----- 从%s加载word2vec模型 -----\" % w2v_model_path)\n",
    "# 这里放上训练时的EpochSaver, 否则load会报错\n",
    "class EpochSaver(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    '''用于保存模型, 打印损失函数等等'''\n",
    "    def __init__(self, savedir, save_name=\"word2vector.model\"):\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        self.save_path = os.path.join(savedir, save_name)\n",
    "        self.epoch = 0\n",
    "        self.pre_loss = 0\n",
    "        self.best_loss = 999999999.9\n",
    "        self.since = time.time()\n",
    "        \n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        cum_loss = model.get_latest_training_loss() # 返回的是从第一个epoch累计的\n",
    "        epoch_loss = cum_loss - self.pre_loss\n",
    "        time_taken = time.time() - self.since\n",
    "        print(\"Epoch %d, loss: %.2f, time: %dmin %ds\" % \n",
    "                    (self.epoch, epoch_loss, time_taken//60, time_taken%60))\n",
    "        if self.best_loss > epoch_loss:\n",
    "            self.best_loss = epoch_loss\n",
    "            print(\"Better model. Best loss: %.2f\" % self.best_loss)\n",
    "            model.save(self.save_path)\n",
    "            print(\"Model %s save done!\" % self.save_path)\n",
    "            \n",
    "        self.pre_loss = cum_loss\n",
    "        self.since = time.time()\n",
    "        print(\"\\n\\n\\n\")\n",
    "w2v_model = Word2Vec.load(w2v_model_path)\n",
    "embed_size = w2v_model.trainables.layer1_size\n",
    "print(\"加载完毕. 总词典数: %d, embeding size: %d\" % (len(w2v_model.wv.vocab), embed_size))\n",
    "\n",
    "\n",
    "# 提取embedding\n",
    "words = list(w2v_model.wv.vocab.keys())\n",
    "word2id = {j:(i+2) for i, j in enumerate(words)}\n",
    "print(len(word2id))\n",
    "\n",
    "embedding_matrix = np.zeros((len(word2id)+2, 100), dtype=np.float32)\n",
    "for word, i in tqdm(word2id.items()):\n",
    "    try:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "    except:\n",
    "        continue\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix[:100])\n",
    "del w2v_model\n",
    "gc.collect()\n",
    "\n",
    "# 提取query和title  训练集和测试集\n",
    "chunksize = 50000000\n",
    "test_rows = 20000000\n",
    "train_query = np.zeros((chunksize, 10), dtype=np.int32)\n",
    "train_title = np.zeros((chunksize, 20), dtype=np.int32)\n",
    "test_query =  np.zeros((test_rows, 10), dtype=np.int32)\n",
    "test_title =  np.zeros((test_rows, 20), dtype=np.int32)\n",
    "\n",
    "train_file_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "test_file_path = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "\n",
    "print('训练集1e')\n",
    "with open(train_file_path) as csvfile:\n",
    "    for index,line in tqdm(enumerate(csv.reader(csvfile))):\n",
    "        query = line[1].strip().split()\n",
    "        query = [word2id.get(w, 1) for w in query]\n",
    "        query = query[:10] + [0]*(10-len(query)) if len(query)<10 else query[:10]\n",
    "        query = np.array(query, dtype=np.int32)\n",
    "        train_query[index] = query\n",
    "        \n",
    "        title = line[3].strip().split()\n",
    "        title = [word2id.get(w, 1) for w in title]\n",
    "        title = title[:20] + [0]*(20-len(title)) if len(title)<20 else title[:20]\n",
    "        title = np.array(title, dtype=np.int32)\n",
    "        train_title[index] = title\n",
    "        if index == chunksize - 1: break\n",
    "\n",
    "print('测试集2kw')\n",
    "since = time.time()\n",
    "with open(test_file_path) as csvfile:\n",
    "    for index,line in enumerate(csv.reader(csvfile)):\n",
    "        query = line[1].strip().split()\n",
    "        query = [word2id.get(w, 1) for w in query]\n",
    "        query = query[:10] + [0]*(10-len(query)) if len(query)<10 else query[:10]\n",
    "        query = np.array(query, dtype=np.int32)\n",
    "        test_query[index] = query\n",
    "        \n",
    "        title = line[3].strip().split()\n",
    "        title = [word2id.get(w, 1) for w in title]\n",
    "        title = title[:20] + [0]*(20-len(title)) if len(title)<20 else title[:20]\n",
    "        title = np.array(title, dtype=np.int32)\n",
    "        test_title[index] = title\n",
    "        if (index+1)%5000000 == 0:\n",
    "            print(index+1, 'step. Time consumed:', time.time() - since)\n",
    "\n",
    "\n",
    "# 抽取手工特征 for lstm \n",
    "train_files = [\n",
    "    \"/home/kesci/work/feature/feature_1_0.csv.gz\",\n",
    "    # 特征['query_length', 'title_length', 'WordMatchShare','WordMatchShare_query', 'WordMatchShare_title', \n",
    "    # 'LengthDiff', 'LengthDiffRate', 'LengthRatio_qt', 'LengthRatio_tq']\n",
    "    \"/home/kesci/work/feature/feature_2.csv.gz\",\n",
    "    # 特征['NgramJaccardCoef_1', 'NgramJaccardCoef_2', 'NgramJaccardCoef_3', 'NgramJaccardCoef_4']\n",
    "    \"/home/kesci/work/feature/feature_3.csv.gz\",\n",
    "    # 特征['Levenshtein_ratio', 'Levenshtein_distance_char','query_title_common_words', 'common_word_ratio']\n",
    "    \"/home/kesci/work/feature/feature_4.csv.gz\",\n",
    "    # 特征['lcsubstr_len', 'lcseque_len', 'longest_match_size', 'longest_match_ratio']\n",
    "    \"/home/kesci/work/feature/feature_5.csv.gz\",\n",
    "    # 特征['fuzz_qratio', 'fuzz_partial_ratio']\n",
    "    \"/home/kesci/work/feature/feature_6.csv.gz\",\n",
    "    # 特征['fuzz_partial_token_sort_ratio', 'fuzz_token_set_ratio', 'fuzz_token_sort_ratio']\n",
    "    \"/home/kesci/work/feature/feature_7.csv.gz\",\n",
    "    # 特征['query_Entropy', 'title_Entropy', 'query_title_Entropy', 'WordMatchShare_Entropy']\n",
    "    \"/home/kesci/work/feature_4e/feature_1_1_4e.csv.gz\", #不用\n",
    "    # 特征['TFIDFWordMatchShare', 'TFIDFWordMatchShare_query', 'TFIDFWordMatchShare_title']\n",
    "    # \"/home/kesci/work/feature_0th_2e/feature_9_0_0th_2e.csv.gz\",  不用\n",
    "    # 特征['w2v_avg_cosine', 'w2v_avg_cityblock']\n",
    "    # \"/home/kesci/work/feature_0th_2e/feature_9_1_0th_2e.csv.gz\",  不用\n",
    "    # 特征['w2v_avg_minkowski', 'w2v_avg_braycurtis', 'w2v_avg_canberra']\n",
    "    \"/home/kesci/work/feature_4e/feature_10_1th_2e.csv.gz\",  \n",
    "    # 特征['query_title_click', 'query_nunique_title', 'query_click', 'title_nunique_query', 'title_click']\n",
    "    # \"/home/kesci/work/feature_0th_2e/feature_12_0th_5kw.csv.gz\",\n",
    "    # 特征['d2v_cosine', 'd2v_cityblock', 'd2v_minkowski', 'd2v_braycurtis', 'd2v_canberra']\n",
    "    \"/home/kesci/work/feature/feature_train_4e_hc_add.csv.gz\"\n",
    "    # 特征 ['jaccard_similarity', 'qt_coword_query_ratio', 'qt_coword_title_ratio', 'qt_len_mean', \n",
    "    # 'qt_common_word_acc', 'ngram_query_title_precision', 'ngram_query_title_recall', 'ngram_query_title_acc']\n",
    "]\n",
    "\n",
    "testA_files = [\n",
    "    \"/home/kesci/work/feature/feature_1_0_testA.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_2_testA.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_3_testA.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_4_testA.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_5_testA.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_6_testA.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_7_testA.csv.gz\",\n",
    "    \"/home/kesci/work/feature_4e/feature_1_1_4e_testA.csv.gz\",\n",
    "    \"/home/kesci/work/feature_4e/feature_10_1th_2e_testA.csv.gz\",\n",
    "    \"/home/kesci/work/feature/feature_test_2kw_hc_add.csv.gz\"\n",
    "]\n",
    "cols_sum = 0\n",
    "for file in testA_files:\n",
    "    col_len = len(pd.read_csv(file, dtype='float32', nrows=1).columns)\n",
    "    cols_sum = cols_sum + col_len\n",
    "print('manual feature columns size:', cols_sum)\n",
    "\n",
    "TRAIN_ROWS = 100000000\n",
    "TEST_ROWS = 20000000\n",
    "train_fea = np.empty((TRAIN_ROWS, cols_sum), dtype=np.float32)\n",
    "test_fea = np.empty((TEST_ROWS, cols_sum), dtype=np.float32)\n",
    "st = 0\n",
    "since = time.time()\n",
    "for file in train_files:\n",
    "    col_len = len(pd.read_csv(file, dtype='float32', nrows=1).columns)\n",
    "    if file != \"/home/kesci/work/feature_4e/feature_10_1th_2e.csv.gz\":\n",
    "        train_fea[:,st:st+col_len]=np.array(pd.read_csv(file, dtype='float32', skiprows=2*TRAIN_ROWS, \n",
    "                                            nrows=TRAIN_ROWS), dtype=np.float32)\n",
    "    else:\n",
    "        train_fea[:,st:st+col_len]=np.array(pd.read_csv(file, dtype='float32', \n",
    "                                            nrows=TRAIN_ROWS), dtype=np.float32)\n",
    "    st = st + col_len\n",
    "    print(file, ' end. Time consumed: ', time.time() - since)\n",
    "stt = 0\n",
    "for file in testA_files:\n",
    "    col_len = len(pd.read_csv(file, dtype='float32', nrows=1).columns)\n",
    "    test_fea[:,stt:stt+col_len]=np.array(pd.read_csv(file, dtype='float32', nrows=TEST_ROWS),dtype=np.float32)\n",
    "    stt = stt + col_len\n",
    "    print(file, ' end')\n",
    "    \n",
    "train_label_file = \"/home/kesci/work/feature/train_label.csv.gz\"\n",
    "train_y = pd.read_csv(train_label_file, dtype='float32',nrows=50000000)[\"label\"].values.astype(np.int32)\n",
    "print(train_query.shape, train_title.shape, train_fea.shape, train_y.shape)\n",
    "\n",
    "\n",
    "# define nn model (based on lstm) 增大网络容量\n",
    "def pool_corr(q1,q2,pool_way):\n",
    "    if pool_way == 'max':\n",
    "        pool = GlobalMaxPooling1D()\n",
    "    elif pool_way == 'ave':\n",
    "        pool = GlobalAveragePooling1D()\n",
    "    else:\n",
    "        raise RuntimeError(\"don't have this pool way\")\n",
    "    q1 = pool(q1)\n",
    "    q2 = pool(q2)\n",
    "\n",
    "    def norm_layer(x, axis=1):\n",
    "        return (x - K.mean(x, axis=axis, keepdims=True)) / K.std(x, axis=axis, keepdims=True)\n",
    "    q1 = Lambda(norm_layer)(q1)\n",
    "    q2 = Lambda(norm_layer)(q2)\n",
    "    \n",
    "    def jaccard(x):\n",
    "        return  x[0]*x[1]/(K.sum(x[0]**2,axis=1,keepdims=True)+\n",
    "                           K.sum(x[1]**2,axis=1,keepdims=True)-\n",
    "                           K.sum(K.abs(x[0]*x[1]),axis=1,keepdims=True))\n",
    "    merged = Lambda(jaccard)([q1, q2])\n",
    "    return merged\n",
    "\n",
    "query = Input(shape=(10, ), name='query')\n",
    "title = Input(shape=(20, ), name='title')\n",
    "query_mask = Lambda(lambda x: K.cast(K.greater(K.expand_dims(x, 2), 0), 'float32'))(query)\n",
    "title_mask = Lambda(lambda x: K.cast(K.greater(K.expand_dims(x, 2), 0), 'float32'))(title)\n",
    "\n",
    "embedd_word = Embedding(len(word2id)+2, 300, weights=[embedding_matrix], trainable=False, name='emb')\n",
    "\n",
    "lstm_dim1 = 200\n",
    "lstm_dim2 = 150\n",
    "\n",
    "lstm_w = Bidirectional(CuDNNLSTM(lstm_dim1,return_sequences=True), merge_mode='sum')\n",
    "lstm2_w = Bidirectional(CuDNNLSTM(lstm_dim2,return_sequences=True), merge_mode='sum')\n",
    "\n",
    "norm = BatchNormalization()\n",
    "q1 = embedd_word(query)\n",
    "q1 = norm(q1)\n",
    "q1 = SpatialDropout1D(0.2)(q1)\n",
    "\n",
    "q2 = embedd_word(title)\n",
    "q2 = norm(q2)\n",
    "q2 = SpatialDropout1D(0.2)(q2)\n",
    "\n",
    "q1 = Lambda(lambda x: x[0] * x[1])([q1, query_mask])  # mask\n",
    "q2 = Lambda(lambda x: x[0] * x[1])([q2, title_mask])\n",
    "\n",
    "q1 = lstm_w(q1)\n",
    "q2 = lstm_w(q2)\n",
    "q1 = Lambda(lambda x: x[0] * x[1])([q1, query_mask])  # mask\n",
    "q2 = Lambda(lambda x: x[0] * x[1])([q2, title_mask])\n",
    "q1 = lstm2_w(q1)\n",
    "q2 = lstm2_w(q2)\n",
    "\n",
    "merged_max = pool_corr(q1, q2, 'max')\n",
    "merged_ave = pool_corr(q1, q2, 'ave')\n",
    "\n",
    "manual_fea = Input(shape=(46,),name='mf')\n",
    "mf = BatchNormalization()(manual_fea)\n",
    "mf = Dense(100, activation='relu')(mf)\n",
    "mf = Dropout(0.2)(mf)\n",
    "\n",
    "merged = concatenate([merged_ave, merged_max])\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = concatenate([merged, mf])\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "lr=0.001\n",
    "model = Model(inputs=[query, title, manual_fea], outputs=output)\n",
    "model.compile(loss='binary_crossentropy',optimizer=Nadam(lr),metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 模型运行 先训练前一亿得到权重，再加载该权重在一亿数据上进行微调。\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "# lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=3, min_lr=0.5e-6)\n",
    "early_stopper = EarlyStopping(min_delta=0.001, patience=6)\n",
    "weights_file=\"/home/kesci/work/nn_related/0805_lstm_bigsize_bn.weights\"\n",
    "model_checkpoint= ModelCheckpoint(weights_file, monitor=\"val_loss\", \\\n",
    "        save_best_only=True, save_weights_only=True, mode='auto')\n",
    "\n",
    "def schedule_steps(epoch, steps):\n",
    "    for step in steps:\n",
    "        if step[1] > epoch:\n",
    "            print(\"Setting learning rate to {}\".format(step[0]))\n",
    "            return step[0]\n",
    "    print(\"Setting learning rate to {}\".format(steps[-1][0]))\n",
    "    return steps[-1][0]\n",
    "lrSchedule = LearningRateScheduler(lambda epoch: schedule_steps(epoch, [(0.001, 4), (0.000316228, 8), \n",
    "                                                                        (0.0001, 12), (3.16228e-05, 16),\n",
    "                                                                        (1e-05, 20)]))\n",
    "callbacks_list=[early_stopper, model_checkpoint, lrSchedule]\n",
    "\n",
    "model.fit([train_query[:80000000], train_title[:80000000], train_fea[:80000000]], \n",
    "          train_y[:80000000], epochs=30, batch_size=2048,\n",
    "          validation_data=([train_query[80000000:], train_title[80000000:], train_fea[80000000:]], train_y[80000000:]),\n",
    "          callbacks=callbacks_list)\n",
    "\n",
    "'''\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "# lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=3, min_lr=0.5e-6)\n",
    "early_stopper = EarlyStopping(min_delta=0.001, patience=5)\n",
    "weights_file=\"/home/kesci/work/nn_related/0808_lstm_bigsize_bn_.weights\"\n",
    "model_checkpoint= ModelCheckpoint(weights_file, monitor=\"val_loss\", \\\n",
    "        save_best_only=True, save_weights_only=True, mode='auto')\n",
    "\n",
    "def schedule_steps(epoch, steps):\n",
    "\tfor step in steps:\n",
    "\t\tif step[1] > epoch:\n",
    "\t\t\tprint(\"Setting learning rate to {}\".format(step[0]))\n",
    "\t\t\treturn step[0]\n",
    "\tprint(\"Setting learning rate to {}\".format(steps[-1][0]))\n",
    "\treturn steps[-1][0]\n",
    "lrSchedule = LearningRateScheduler(lambda epoch: schedule_steps(epoch, [(0.0005, 2), (0.000316228, 4), \n",
    "                                                                        (0.0001, 6), (3.16228e-05, 8),\n",
    "                                                                        (1e-05, 10)]))\n",
    "callbacks_list=[early_stopper, model_checkpoint, lrSchedule]\n",
    "\n",
    "model.load_weights('/home/kesci/work/nn_related/0805_lstm_bigsize_bn.weights')  \n",
    "model.fit([train_query[:80000000], train_title[:80000000], train_fea[:80000000]], \n",
    "          train_y[:80000000], epochs=20, batch_size=2048,\n",
    "          validation_data=([train_query[80000000:], train_title[80000000:], train_fea[80000000:]], train_y[80000000:]),\n",
    "          callbacks=callbacks_list)\n",
    "'''\n",
    "\n",
    "# 预测  test_data 2kw   （1e测试集与此类似）\n",
    "model.load_weights('/home/kesci/work/nn_related/0808_lstm_bigsize_bn_.weights')\n",
    "test_preds = model.predict([test_query, test_title, test_fea], batch_size=1024, verbose=1) \n",
    "\n",
    "# 保存预测文件\n",
    "submission_df = pd.read_csv('0802_lgb_hc_all_fea_cvr.csv', header=None, names=['1', '2', '3'])[['1', '2']]\n",
    "print(submission_df.info())\n",
    "submission_df[\"3\"] = test_preds[:,0]\n",
    "print(submission_df.head(5))\n",
    "save_file = '/home/kesci/work/nn_related/nn_submit/0809_1e_hc_300d_nn_pretrain.csv'\n",
    "submission_df.to_csv(save_file, index=False, header=False)\n",
    "print(\"%s Save Done!\" % save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D75325A49E9D47228CA590ED9C7D5748"
   },
   "source": [
    "# 5 最终提交代码\n",
    "* 将得到的结果进行加权平均并按照要求保存提交文件\n",
    "* 下面代码是2千万的，1亿的类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "EEB7B7EE2EB644FB82F14026AEDFBCD2"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "test_file_path = \"/home/kesci/input/bytedance/test_final_part1.csv\"\n",
    "TEST_ROWS = 20000000\n",
    "\n",
    "# 每个文件包含一列，存放了2千万测试集的prediction\n",
    "files = [\n",
    "\"/home/kesci/work/lgb_model_result/805_tss_lgb_testA_0-9_avg.csv.gz\", # 0.60508400\n",
    "\"/home/kesci/work/lgb_model_result/808_hc_lgb_testA_avg.csv.gz\", # 0.60487700\n",
    "\n",
    "\"/home/kesci/work/nn_related/tss_esim_806_4077.csv.gz\", # 0.61537900\n",
    "\"/home/kesci/work/nn_related/tss_esim_809_4181.csv.gz\", # 0.61660700\n",
    "\"/home/kesci/work/nn_related/tss_esim100_811_4146.csv.gz\", # 0.62665700\n",
    "\n",
    "\n",
    "\"/home/kesci/work/nn_related/nn_submit/0806_1e_hc_300d_nn.csv\", # 0.61092100\n",
    "\"/home/kesci/work/nn_related/nn_submit/0809_1e_hc_300d_nn_pretrain.csv\", # 0.61357200\n",
    "\n",
    "]\n",
    "\n",
    "weights = [\n",
    "2, 2, 5, 6, 18, 3, 4\n",
    "]\n",
    "\n",
    "if len(weights) == 0: # 如果没给权重那就设权重一样\n",
    "    print(\"直接平均.\")\n",
    "    weights = list([1 for _ in range(len(files))])\n",
    "else:\n",
    "    print(weights)\n",
    "assert len(weights) == len(files)\n",
    "\n",
    "\n",
    "save_file = \"/home/kesci/work/submit/wieght_225634_18.csv\"\n",
    "\n",
    "\n",
    "df_for_corr = pd.DataFrame()\n",
    "\n",
    "# 加权平均\n",
    "preds = np.zeros((TEST_ROWS,), dtype=np.float32)\n",
    "for i in range(len(files)):\n",
    "    preds_csv = pd.read_csv(files[i], header=None, \n",
    "                            names=[\"prediction\"], dtype=\"float32\")\n",
    "    preds += weights[i] * preds_csv[\"prediction\"].values\n",
    "    \n",
    "    # 用前100000行计算皮尔森相关系数\n",
    "    n = 1000000\n",
    "    df_for_corr[\"%d\" % i] = preds_csv[\"prediction\"].values[:n]\n",
    "    \n",
    "preds /= sum(weights)\n",
    "\n",
    "\n",
    "# 相关系数可视化\n",
    "import seaborn as sns\n",
    "plt.subplots(figsize=(df_for_corr.shape[1], df_for_corr.shape[1]))\n",
    "sns.heatmap(abs(df_for_corr.corr().values), annot=True, vmax=1, square=True, cmap=\"Blues\", cbar=False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 加上两行: query id 和 query_title_id\n",
    "test_columns = ['query_id','query','query_title_id','title']\n",
    "submission_df = pd.read_csv(test_file_path, names = test_columns,\n",
    "                            dtype={\"query_id\":np.int32, \"query\":str, \n",
    "                                    \"query_title_id\":np.int32, \n",
    "                                    \"title\":str})[[\"query_id\", \"query_title_id\"]]\n",
    "submission_df[\"prediction\"] = preds\n",
    "submission_df.to_csv(save_file, index=False, header=False)\n",
    "print(\"%s Save Done!\" % save_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
